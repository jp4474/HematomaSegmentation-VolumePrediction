{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\nimport monai\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:10.597946Z","iopub.execute_input":"2024-04-15T18:15:10.598203Z","iopub.status.idle":"2024-04-15T18:15:41.859974Z","shell.execute_reply.started":"2024-04-15T18:15:10.598179Z","shell.execute_reply":"2024-04-15T18:15:41.859113Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-15 18:15:39.594131: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-15 18:15:39.594187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-15 18:15:39.595672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# !wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:46.680139Z","iopub.execute_input":"2024-04-15T18:15:46.680933Z","iopub.status.idle":"2024-04-15T18:15:46.688048Z","shell.execute_reply.started":"2024-04-15T18:15:46.680901Z","shell.execute_reply":"2024-04-15T18:15:46.687089Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:41.944064Z","iopub.execute_input":"2024-04-15T18:15:41.944315Z","iopub.status.idle":"2024-04-15T18:15:41.949024Z","shell.execute_reply.started":"2024-04-15T18:15:41.944291Z","shell.execute_reply":"2024-04-15T18:15:41.947995Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:49.820834Z","iopub.execute_input":"2024-04-15T18:15:49.821462Z","iopub.status.idle":"2024-04-15T18:15:49.846878Z","shell.execute_reply.started":"2024-04-15T18:15:49.821432Z","shell.execute_reply":"2024-04-15T18:15:49.845878Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:51.877467Z","iopub.execute_input":"2024-04-15T18:15:51.877838Z","iopub.status.idle":"2024-04-15T18:15:52.017396Z","shell.execute_reply.started":"2024-04-15T18:15:51.877810Z","shell.execute_reply":"2024-04-15T18:15:52.016620Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load MedSAM","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:57.631763Z","iopub.execute_input":"2024-04-15T18:15:57.632141Z","iopub.status.idle":"2024-04-15T18:15:57.640994Z","shell.execute_reply.started":"2024-04-15T18:15:57.632110Z","shell.execute_reply":"2024-04-15T18:15:57.639871Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:15:59.823499Z","iopub.execute_input":"2024-04-15T18:15:59.823919Z","iopub.status.idle":"2024-04-15T18:15:59.831659Z","shell.execute_reply.started":"2024-04-15T18:15:59.823887Z","shell.execute_reply":"2024-04-15T18:15:59.830539Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    W = 240\n    H = 240\n    margin = 5\n    box_1024_batch = []\n    useless_image_indices = []\n    \n    for ind, mask in enumerate(batch_mask):\n        mask = mask[0]\n        nonzero_indices = torch.nonzero(mask).T\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n#             #box_1024 = torch.asarray([[-1, -1, -1, -1]])\n        else:\n            min_y = torch.min(nonzero_indices[0, :]).values\n            max_y = torch.max(nonzero_indices[0, :]).values\n            min_x = torch.min(nonzero_indices[1, :]).values\n            max_x = torch.max(nonzero_indices[1, :]).values\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n            box_1024_batch.append(box_1024)\n    \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:16:01.859562Z","iopub.execute_input":"2024-04-15T18:16:01.859947Z","iopub.status.idle":"2024-04-15T18:16:01.869384Z","shell.execute_reply.started":"2024-04-15T18:16:01.859915Z","shell.execute_reply":"2024-04-15T18:16:01.868490Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:16:03.807008Z","iopub.execute_input":"2024-04-15T18:16:03.807775Z","iopub.status.idle":"2024-04-15T18:16:03.815686Z","shell.execute_reply.started":"2024-04-15T18:16:03.807735Z","shell.execute_reply":"2024-04-15T18:16:03.814704Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# #%% load model and image\n# MedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\n# device = \"cuda:0\"\n# medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n# medsam_model = medsam_model.to(device)\n# medsam_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5, W = 240, H = 240):\n    box_1024_batch = []\n    \n    for ind, mask in enumerate(batch_mask):\n        mask = mask[0]\n        nonzero_indices = torch.nonzero(mask).T\n        \n        #if(nonzero_indices.shape[-1] == 0):\n        min_y = torch.min(nonzero_indices[0, :])\n        max_y = torch.max(nonzero_indices[0, :])\n        min_x = torch.min(nonzero_indices[1, :])\n        max_x = torch.max(nonzero_indices[1, :])\n\n        x0 = min_x - margin\n        y0 = min_y - margin\n        x1 = max_x + margin\n        y1 = max_y + margin\n\n        box = torch.asarray([[x0,y0, x1, y1]])\n        box = box / torch.asarray([W, H, W, H]) * 1024\n        box_1024_batch.append(box)\n    \n    box_batch = torch.cat(box_1024_batch, dim = 0)\n    box_batch = box_batch.unsqueeze(dim = 1)\n    return box_batch\n\ndef generate_box_and_embedding(img, mask, W = 240, H = 240):\n\n    # resize image, make it 3D\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img = resizer(img)\n    img = img.repeat(1, 3, 1, 1)\n    \n    #mask = resizer(mask)\n    # generate bounding boxes\n    box_batch = box_coordinates_from_mask_batch(mask)\n  \n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(img) # (1, 256, 64, 64)\n        \n    return image_embedding, mask, box_batch","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:16:05.637361Z","iopub.execute_input":"2024-04-15T18:16:05.638247Z","iopub.status.idle":"2024-04-15T18:16:05.647979Z","shell.execute_reply.started":"2024-04-15T18:16:05.638213Z","shell.execute_reply":"2024-04-15T18:16:05.646992Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#MedSAM train class\nclass MedSAM(nn.Module):\n    def __init__(\n        self,\n        image_encoder,\n        mask_decoder,\n        prompt_encoder,\n    ):\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.mask_decoder = mask_decoder\n        self.prompt_encoder = prompt_encoder\n        # freeze prompt encoder\n        for param in self.prompt_encoder.parameters():\n            param.requires_grad = False\n\n    def forward(self, image, box):\n        image_embedding = self.image_encoder(image)  # (B, 256, 64, 64)\n        # do not compute gradients for prompt encoder\n        with torch.no_grad():\n            box_torch = torch.as_tensor(box, dtype=torch.float32, device=image.device)\n            if len(box_torch.shape) == 2:\n                box_torch = box_torch[:, None, :]  # (B, 1, 4)\n\n            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n                points=None,\n                boxes=box_torch,\n                masks=None,\n            )\n        low_res_masks, _ = self.mask_decoder(\n            image_embeddings=image_embedding,  # (B, 256, 64, 64)\n            image_pe=self.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n            multimask_output=False,\n        )\n        ori_res_masks = F.interpolate(\n            low_res_masks,\n            size=(image.shape[2], image.shape[3]),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        return ori_res_masks","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:16:07.811111Z","iopub.execute_input":"2024-04-15T18:16:07.811509Z","iopub.status.idle":"2024-04-15T18:16:07.821167Z","shell.execute_reply.started":"2024-04-15T18:16:07.811480Z","shell.execute_reply":"2024-04-15T18:16:07.820234Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Medsam Train code\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\nsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = MedSAM(\n    image_encoder=sam_model.image_encoder,\n    mask_decoder=sam_model.mask_decoder,\n    prompt_encoder=sam_model.prompt_encoder,\n).to(device)\nmedsam_model.train()\nmedsam_model = medsam_model.to(device)\n# Useless print stuff\n# print(\n#     \"Number of total parameters: \",\n#     sum(p.numel() for p in medsam_model.parameters()),\n# )  # 93735472\n# print(\n#     \"Number of trainable parameters: \",\n#     sum(p.numel() for p in medsam_model.parameters() if p.requires_grad),\n# )  # 93729252\n\n# img_mask_encdec_params = list(medsam_model.image_encoder.parameters()) + list(\n#     medsam_model.mask_decoder.parameters()\n# )\n# optimizer = torch.optim.AdamW(\n#     img_mask_encdec_params, lr=1e-4, weight_decay=0.01\n# )\n# print(\n#     \"Number of image encoder and mask decoder parameters: \",\n#     sum(p.numel() for p in img_mask_encdec_params if p.requires_grad),\n# )  # 93729252\n\n# Important hyperparameters and initialization\n# seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n# cross entropy loss\nce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n# %% train\nnum_epochs = 0\niter_num = 0\nlosses = []\nbest_loss = 1e10\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:17:19.747954Z","iopub.execute_input":"2024-04-15T18:17:19.748680Z","iopub.status.idle":"2024-04-15T18:17:21.237244Z","shell.execute_reply.started":"2024-04-15T18:17:19.748638Z","shell.execute_reply":"2024-04-15T18:17:21.236180Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# train_dataset = NpyDataset(args.tr_npy_path)\n\n# print(\"Number of training samples: \", len(train_dataset))\n# train_dataloader = DataLoader(\n#     train_dataset,\n#     batch_size=args.batch_size,\n#     shuffle=True,\n#     num_workers=args.num_workers,\n#     pin_memory=True,\n# )\n\n# start_epoch = 0\n# if args.resume is not None:\n#     if os.path.isfile(args.resume):\n#         ## Map model to be loaded to specified single GPU\n#         checkpoint = torch.load(args.resume, map_location=device)\n#         start_epoch = checkpoint[\"epoch\"] + 1\n#         medsam_model.load_state_dict(checkpoint[\"model\"])\n#         optimizer.load_state_dict(checkpoint[\"optimizer\"])\n# if args.use_amp:\n#     scaler = torch.cuda.amp.GradScaler()\n\n# for epoch in range(start_epoch, num_epochs):\n#     epoch_loss = 0\n#     for step, (image, gt2D, boxes, _) in enumerate(tqdm(train_dataloader)):\n#         optimizer.zero_grad()\n#         boxes_np = boxes.detach().cpu().numpy()\n#         image, gt2D = image.to(device), gt2D.to(device)\n#         if args.use_amp:\n#             ## AMP\n#             with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n#                 medsam_pred = medsam_model(image, boxes_np)\n#                 loss = seg_loss(medsam_pred, gt2D) + ce_loss(\n#                     medsam_pred, gt2D.float()\n#                 )\n#             scaler.scale(loss).backward()\n#             scaler.step(optimizer)\n#             scaler.update()\n#             optimizer.zero_grad()\n#         else:\n#             medsam_pred = medsam_model(image, boxes_np)\n#             loss = seg_loss(medsam_pred, gt2D) + ce_loss(medsam_pred, gt2D.float())\n#             loss.backward()\n#             optimizer.step()\n#             optimizer.zero_grad()\n\n#         epoch_loss += loss.item()\n#         iter_num += 1\n\n#     epoch_loss /= step\n#     losses.append(epoch_loss)\n#     if args.use_wandb:\n#         wandb.log({\"epoch_loss\": epoch_loss})\n#     print(\n#         f'Time: {datetime.now().strftime(\"%Y%m%d-%H%M\")}, Epoch: {epoch}, Loss: {epoch_loss}'\n#     )\n#     ## save the latest model\n#     checkpoint = {\n#         \"model\": medsam_model.state_dict(),\n#         \"optimizer\": optimizer.state_dict(),\n#         \"epoch\": epoch,\n#     }\n#     torch.save(checkpoint, join(model_save_path, \"medsam_model_latest.pth\"))\n#     ## save the best model\n#     if epoch_loss < best_loss:\n#         best_loss = epoch_loss\n#         checkpoint = {\n#             \"model\": medsam_model.state_dict(),\n#             \"optimizer\": optimizer.state_dict(),\n#             \"epoch\": epoch,\n#         }\n#         torch.save(checkpoint, join(model_save_path, \"medsam_model_best.pth\"))\n\n#     # %% plot loss\n#     plt.plot(losses)\n#     plt.title(\"Dice + Cross Entropy Loss\")\n#     plt.xlabel(\"Epoch\")\n#     plt.ylabel(\"Loss\")\n#     plt.savefig(join(model_save_path, args.task_name + \"train_loss.png\"))\n#     plt.close()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Medsam inference code\n# @torch.no_grad()\n# def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n#     box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n#     if len(box_torch.shape) == 2:\n#         box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n#     sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n#         points=None,\n#         boxes=box_torch,\n#         masks=None,\n#     )\n    \n# #     print(f'img_embed: {img_embed.shape} should be (B, 256, 64, 64)')\n# #     print(f'sparse_embeddings : {sparse_embeddings.shape} (B, 2, 256)')\n# #     print(f'dense_prompt_embeddings : {dense_embeddings.shape} (B, 256, 64, 64)')\n#     low_res_logits, _ = medsam_model.mask_decoder(\n#         image_embeddings=img_embed, # (B, 256, 64, 64)\n#         image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n#         sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n#         dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n#         multimask_output=False,\n#         )\n\n#     low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n#     low_res_pred = F.interpolate(\n#         low_res_pred,\n#         size=(H, W),\n#         mode=\"bilinear\",\n#         align_corners=False,\n#     )  # (1, 1, gt.shape)\n#     low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n#     medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n#     return medsam_seg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice().to(device)\njaccard_index_metric = BinaryJaccardIndex().to(device)\nwith torch.no_grad():\n    H = 240\n    W = 240\n    for i, batch in enumerate(tqdm(test_dataloader)):\n        # get image and masks from dataloader\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'].to(device).float()\n        \n        indices_to_remove = []\n        \n        for idx, img_mask in enumerate(img_masks):\n            if (torch.sum(img_mask) == 0):\n                indices_to_remove.append(idx)\n        \n        if(len(indices_to_remove) != BATCH_SIZE):\n            indices_to_select = set(range(0, BATCH_SIZE)).difference(set(indices_to_remove))\n            indices_to_select = list(indices_to_select)\n\n            imgs = imgs[indices_to_select,:,:,:]\n            img_masks = img_masks[indices_to_select,:,:,:]\n\n            image_embedding, true_mask, box = generate_box_and_embedding(imgs, img_masks)\n             \n            true_mask = true_mask.int()\n            true_mask = true_mask[0][0]\n            # medsam inference pred\n#             y_pred = medsam_inference(medsam_model, image_embedding, box, H, W)\n\n            # medsam train pred\n            imgs = torch.rand(1,3,256,256).to(device).float()\n            box = torch.rand(1,1,4)\n            print(\"box shape\", box.shape)\n            print(\"imgs shape\", imgs.shape)\n#             imgs = imgs.repeat(1, 3, 1, 1)\n            \n            y_pred = medsam_model(imgs, box)\n            y_pred = torch.from_numpy(y_pred)\n            #y_pred = y_pred.unsqueeze(dim = 1)\n            \n            y_pred = y_pred.to(device)\n            true_mask = true_mask.to(device)\n\n            batch_dice_score = dice_metric(y_pred, true_mask)\n            valid_batch_dice.append(batch_dice_score)\n\n            batch_jaccard_score = jaccard_index_metric(y_pred, true_mask)\n            valid_batch_jaccard.append(batch_jaccard_score)\n\n            print(f'DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')\n    print(f'Validation DICE score: {np.mean(valid_batch_dice)}, Validation Jaccard score: {np.mean(jaccard_index_metric)}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:31:44.072893Z","iopub.execute_input":"2024-04-15T18:31:44.073703Z","iopub.status.idle":"2024-04-15T18:31:44.580833Z","shell.execute_reply.started":"2024-04-15T18:31:44.073668Z","shell.execute_reply":"2024-04-15T18:31:44.579453Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28985 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acbe351e81004bc9805e2b0a54731179"}},"metadata":{}},{"name":"stdout","text":"box shape torch.Size([1, 1, 4])\nimgs shape torch.Size([1, 3, 256, 256])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, imgs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             imgs = imgs.repeat(1, 3, 1, 1)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmedsam_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_pred)\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m#y_pred = y_pred.unsqueeze(dim = 1)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mMedSAM.forward\u001b[0;34m(self, image, box)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, box):\n\u001b[0;32m---> 18\u001b[0m     image_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, 256, 64, 64)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# do not compute gradients for prompt encoder\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:109\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    112\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x)\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (64) at non-singleton dimension 2"],"ename":"RuntimeError","evalue":"The size of tensor a (16) must match the size of tensor b (64) at non-singleton dimension 2","output_type":"error"}]},{"cell_type":"code","source":"train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard = train_net(ddp_model, EPOCHS, train_dataloader, validation_dataloader, optimizer, loss_function)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}