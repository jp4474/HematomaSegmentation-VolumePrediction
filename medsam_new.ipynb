{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:53:29.304068Z","iopub.execute_input":"2024-04-13T17:53:29.304762Z","iopub.status.idle":"2024-04-13T17:53:40.297102Z","shell.execute_reply.started":"2024-04-13T17:53:29.304723Z","shell.execute_reply":"2024-04-13T17:53:40.296115Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:53:48.787194Z","iopub.execute_input":"2024-04-13T17:53:48.787571Z","iopub.status.idle":"2024-04-13T17:54:05.146054Z","shell.execute_reply.started":"2024-04-13T17:53:48.787537Z","shell.execute_reply":"2024-04-13T17:54:05.144956Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-04-13 17:53:49--  https://zenodo.org/records/10689643/files/medsam_vit_b.pth\nResolving zenodo.org (zenodo.org)... 188.184.103.159, 188.184.98.238, 188.185.79.172, ...\nConnecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 375049145 (358M) [application/octet-stream]\nSaving to: 'medsam_vit_b.pth'\n\nmedsam_vit_b.pth    100%[===================>] 357.67M  31.9MB/s    in 15s     \n\n2024-04-13 17:54:05 (24.2 MB/s) - 'medsam_vit_b.pth' saved [375049145/375049145]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:53:40.301666Z","iopub.execute_input":"2024-04-13T17:53:40.301963Z","iopub.status.idle":"2024-04-13T17:53:40.309272Z","shell.execute_reply.started":"2024-04-13T17:53:40.301930Z","shell.execute_reply":"2024-04-13T17:53:40.308410Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:53:40.311384Z","iopub.execute_input":"2024-04-13T17:53:40.311645Z","iopub.status.idle":"2024-04-13T17:53:40.332061Z","shell.execute_reply.started":"2024-04-13T17:53:40.311624Z","shell.execute_reply":"2024-04-13T17:53:40.331205Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:55:09.082721Z","iopub.execute_input":"2024-04-13T17:55:09.083502Z","iopub.status.idle":"2024-04-13T17:55:09.111867Z","shell.execute_reply.started":"2024-04-13T17:55:09.083467Z","shell.execute_reply":"2024-04-13T17:55:09.111008Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:55:43.549735Z","iopub.execute_input":"2024-04-13T17:55:43.550400Z","iopub.status.idle":"2024-04-13T17:55:43.696878Z","shell.execute_reply.started":"2024-04-13T17:55:43.550367Z","shell.execute_reply":"2024-04-13T17:55:43.696044Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for test in val_dataloader:\n    print(test)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:56:40.335728Z","iopub.execute_input":"2024-04-13T17:56:40.336470Z","iopub.status.idle":"2024-04-13T17:56:42.275316Z","shell.execute_reply.started":"2024-04-13T17:56:40.336437Z","shell.execute_reply":"2024-04-13T17:56:42.274377Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'image': tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]],\n\n\n        [[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]],\n\n\n        [[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]],\n\n\n        [[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.int16), 'mask': tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]],\n\n\n        [[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]],\n\n\n        [[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]],\n\n\n        [[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.int16), 'img_id': ['01346_slice_9.npy', '01102_slice_108.npy', '01562_slice_74.npy', '00296_slice_54.npy']}\n","output_type":"stream"}]},{"cell_type":"code","source":"test.keys()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:57:07.191739Z","iopub.execute_input":"2024-04-13T17:57:07.192570Z","iopub.status.idle":"2024-04-13T17:57:07.199020Z","shell.execute_reply.started":"2024-04-13T17:57:07.192516Z","shell.execute_reply":"2024-04-13T17:57:07.197979Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"dict_keys(['image', 'mask', 'img_id'])"},"metadata":{}}]},{"cell_type":"code","source":"test['mask'].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:57:37.404917Z","iopub.execute_input":"2024-04-13T17:57:37.405782Z","iopub.status.idle":"2024-04-13T17:57:37.411453Z","shell.execute_reply.started":"2024-04-13T17:57:37.405749Z","shell.execute_reply":"2024-04-13T17:57:37.410495Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 1, 240, 240])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load MedSAM","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:58:02.191484Z","iopub.execute_input":"2024-04-13T17:58:02.192312Z","iopub.status.idle":"2024-04-13T17:58:02.196323Z","shell.execute_reply.started":"2024-04-13T17:58:02.192277Z","shell.execute_reply":"2024-04-13T17:58:02.195435Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:58:08.885718Z","iopub.execute_input":"2024-04-13T17:58:08.886112Z","iopub.status.idle":"2024-04-13T17:58:08.895124Z","shell.execute_reply.started":"2024-04-13T17:58:08.886081Z","shell.execute_reply":"2024-04-13T17:58:08.894252Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:02.767312Z","iopub.execute_input":"2024-04-12T22:40:02.767594Z","iopub.status.idle":"2024-04-12T22:40:02.781020Z","shell.execute_reply.started":"2024-04-12T22:40:02.767567Z","shell.execute_reply":"2024-04-12T22:40:02.780203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    counter = 0\n    W = 240\n    H = 240\n    box_1024_batch = []\n    useless_image_indices = []\n    for ind, mask in enumerate(batch_mask):\n#         print(\"mask shape\", mask.shape)\n        mask = mask[0]\n#         print(\"mask shape 2\", mask.shape)\n        nonzero_indices = torch.nonzero(mask).T\n#         print(\"nonzero_indices shape:\", nonzero_indices.shape)\n#         nonzero_indices = torch.transpose(torch.nonzero(mask).T)\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n            box_1024 = torch.asarray([-1, -1, -1, -1])\n#             x0 = 0\n#             y0 = 0\n#             x1 = \n        else:\n            min_y = torch.min(nonzero_indices[0, :])\n            max_y = torch.max(nonzero_indices[0, :])\n            min_x = torch.min(nonzero_indices[1, :])\n            max_x = torch.max(nonzero_indices[1, :])\n\n            margin = 5\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n            # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n        box_1024_batch.append(box_1024)\n\n    if counter == BATCH_SIZE:\n        return         \n        \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n\n    print(final_tensor.shape)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:44:36.834078Z","iopub.execute_input":"2024-04-12T22:44:36.834504Z","iopub.status.idle":"2024-04-12T22:44:36.846716Z","shell.execute_reply.started":"2024-04-12T22:44:36.834473Z","shell.execute_reply":"2024-04-12T22:44:36.845769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    # print(\"x_0, y_0\", x0, y0)\n    # print(\"w, h\", w, h)\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))\n\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n    if len(box_torch.shape) == 2:\n        box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:02.793514Z","iopub.execute_input":"2024-04-12T22:40:02.793761Z","iopub.status.idle":"2024-04-12T22:40:02.807920Z","shell.execute_reply.started":"2024-04-12T22:40:02.793739Z","shell.execute_reply":"2024-04-12T22:40:02.806997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda:0\"\nmedsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = medsam_model.to(device)\nmedsam_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:02.824247Z","iopub.execute_input":"2024-04-12T22:40:02.824524Z","iopub.status.idle":"2024-04-12T22:40:04.400078Z","shell.execute_reply.started":"2024-04-12T22:40:02.824502Z","shell.execute_reply":"2024-04-12T22:40:04.399118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_box_and_embedding(img, img_mask):\n    H = 240\n    W = 240\n\n    #%% image preprocessing part 2\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img_1024 = resizer(img)\n    print(\"img_1024 shape: \", img_1024.shape)\n\n    # need to rewrite box coordinates from mask as input is now [16,1,240,240]\n#     x0, y0, x1, y1 = box_coordinates_from_mask(mask_np)\n#     box_np = np.array([[x0,y0, x1, y1]])\n#     box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    box_1024, useless_image_indices = box_coordinates_from_mask_batch(img_mask)\n    print(img_1024.shape)\n    \n    splicer = torch.ones(BATCH_SIZE, dtype=bool)\n    for index in useless_image_indices:\n        splicer[index] = False\n    # Negate the mask\n    negated_splicer = ~splicer\n#     print(\"negated mask\", negated_mask)\n#     print(\"img_1024.shape\", img_1024.shape)\n    filtered_img_1024_tensor = img_1024[negated_splicer,:,:,:]    \n    \n    filtered_img_1024_tensor_repeated = filtered_img_1024_tensor.repeat(1, 3, 1, 1)\n    print(\"filtered shape\", filtered_img_1024_tensor.shape)\n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(filtered_img_1024_tensor_repeated) # (1, 256, 64, 64)\n    print(\"image_embedding shape\", image_embedding.shape)\n    print(\"box_1024 shape\", box_1024.shape)\n    return image_embedding, box_1024, negated_mask","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:04.401519Z","iopub.execute_input":"2024-04-12T22:40:04.402274Z","iopub.status.idle":"2024-04-12T22:40:04.411536Z","shell.execute_reply.started":"2024-04-12T22:40:04.402237Z","shell.execute_reply":"2024-04-12T22:40:04.410534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epoch = 0\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\nwith torch.no_grad():\n    H = 240\n    W = 240\n    for i, batch in enumerate(tqdm(validation_dataloader)):\n        imgs = batch['image'].to(device).float()\n        true_masks = batch['mask'] #.to(device).float()\n\n#         y_pred = net(imgs).cpu()\n        image_embedding, box_1024, negated_mask = generate_box_and_embedding(imgs, true_masks)\n        y_pred = torch.from_numpy(medsam_inference(medsam_model, image_embedding, box_1024, H, W)).unsqueeze(dim = 1)\n        \n        filtered_true_masks = true_masks[negated_mask,:,:,:]\n        \n        print(\"preds shape\", y_pred.shape)\n        print(\"mask shape\", filtered_true_masks.shape)\n        \n        batch_dice_score = dice_metric(y_pred, filtered_true_masks)\n        print(\"Dice\", batch_dice_score)\n        valid_batch_dice.append(batch_dice_score)\n\n        batch_jaccard_score = jaccard_index_metric(y_pred, filtered_true_masks)\n        valid_batch_jaccard.append(batch_jaccard_score)\n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:04.412877Z","iopub.execute_input":"2024-04-12T22:40:04.413156Z","iopub.status.idle":"2024-04-12T22:40:05.033233Z","shell.execute_reply.started":"2024-04-12T22:40:04.413133Z","shell.execute_reply":"2024-04-12T22:40:05.032031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, batch in enumerate(validation_dataloader):\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:05.034037Z","iopub.status.idle":"2024-04-12T22:40:05.034402Z","shell.execute_reply.started":"2024-04-12T22:40:05.034229Z","shell.execute_reply":"2024-04-12T22:40:05.034244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard = train_net(ddp_model, EPOCHS, train_dataloader, validation_dataloader, optimizer, loss_function)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:40:05.035459Z","iopub.status.idle":"2024-04-12T22:40:05.035812Z","shell.execute_reply.started":"2024-04-12T22:40:05.035637Z","shell.execute_reply":"2024-04-12T22:40:05.035652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#fixme: ","metadata":{}},{"cell_type":"markdown","source":"## Old code","metadata":{}},{"cell_type":"code","source":"# def generate_box_and_embedding(img_np, mask_np):\n# #     slice_indexes.append(slice_ind)\n# #     if len(img_np.shape) == 2:\n# #         img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n# #     else:\n# #         img_3c = img_np\n# #     H, W, _ = img_3c.shape\n#     H = 240\n#     W = 240\n\n#     #%% image preprocessing part 2\n#     # 16,1,1024,1024\n#     resizer = torchvision.transforms.Resize(size = (1024,1024))\n#     img_1024 = resizer(img_np)\n# #     img_1024 = transform.resize(img_np, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n#     print(\"img_1024 shape: \", img_1024.shape)\n# #     img_1024 = (img_1024 - img_1024.min()) / np.clip(\n# #         img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n# #     )  # normalize to [0, 1], (H, W, 3)\n# #     # convert the shape to (3, H, W)\n# #     img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n#     # need to rewrite box coordinates from mask as input is now [16,1,240,240]\n# #     x0, y0, x1, y1 = box_coordinates_from_mask(mask_np)\n# #     box_np = np.array([[x0,y0, x1, y1]])\n# #     box_1024 = box_np / np.array([W, H, W, H]) * 1024\n#     box_1024, useless_image_indices = box_coordinates_from_mask_batch(mask_np)\n#     print(img_1024.shape)\n    \n#     mask = torch.ones(BATCH_SIZE, dtype=bool)\n#     for index in useless_image_indices:\n#         mask[index] = False\n#     # Negate the mask\n#     negated_mask = ~mask\n# #     print(\"negated mask\", negated_mask)\n# #     print(\"img_1024.shape\", img_1024.shape)\n#     filtered_img_1024_tensor = img_1024[negated_mask,:,:,:]    \n    \n#     filtered_img_1024_tensor_repeated = filtered_img_1024_tensor.repeat(1, 3, 1, 1)\n#     print(\"filtered shape\", filtered_img_1024_tensor.shape)\n#     with torch.no_grad():\n#         image_embedding = medsam_model.image_encoder(filtered_img_1024_tensor_repeated) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n#     return image_embedding, box_1024, negated_mask","metadata":{},"execution_count":null,"outputs":[]}]}