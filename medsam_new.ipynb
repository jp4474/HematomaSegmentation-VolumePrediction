{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import libraries, import medsam checkpoint and define H, W, batch size","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:01.700659Z","iopub.execute_input":"2024-04-13T19:35:01.701396Z","iopub.status.idle":"2024-04-13T19:35:07.909264Z","shell.execute_reply.started":"2024-04-13T19:35:01.701361Z","shell.execute_reply":"2024-04-13T19:35:07.908184Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"H = 240\nW = 240\nBATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:07.910413Z","iopub.execute_input":"2024-04-13T19:35:07.910854Z","iopub.status.idle":"2024-04-13T19:35:07.915305Z","shell.execute_reply.started":"2024-04-13T19:35:07.910829Z","shell.execute_reply":"2024-04-13T19:35:07.914276Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda:0\"\nmedsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = medsam_model.to(device)\nmedsam_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:07.916458Z","iopub.execute_input":"2024-04-13T19:35:07.916773Z","iopub.status.idle":"2024-04-13T19:35:09.473233Z","shell.execute_reply.started":"2024-04-13T19:35:07.916748Z","shell.execute_reply":"2024-04-13T19:35:09.472318Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# !wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.477302Z","iopub.execute_input":"2024-04-13T19:35:09.477686Z","iopub.status.idle":"2024-04-13T19:35:09.482879Z","shell.execute_reply.started":"2024-04-13T19:35:09.477647Z","shell.execute_reply":"2024-04-13T19:35:09.481657Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 2. Define Dataset Class, load data","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.484307Z","iopub.execute_input":"2024-04-13T19:35:09.485070Z","iopub.status.idle":"2024-04-13T19:35:09.493993Z","shell.execute_reply.started":"2024-04-13T19:35:09.485039Z","shell.execute_reply":"2024-04-13T19:35:09.493175Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.495324Z","iopub.execute_input":"2024-04-13T19:35:09.495853Z","iopub.status.idle":"2024-04-13T19:35:09.505797Z","shell.execute_reply.started":"2024-04-13T19:35:09.495819Z","shell.execute_reply":"2024-04-13T19:35:09.504024Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.507532Z","iopub.execute_input":"2024-04-13T19:35:09.507979Z","iopub.status.idle":"2024-04-13T19:35:09.516755Z","shell.execute_reply.started":"2024-04-13T19:35:09.507945Z","shell.execute_reply":"2024-04-13T19:35:09.515675Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.518126Z","iopub.execute_input":"2024-04-13T19:35:09.518617Z","iopub.status.idle":"2024-04-13T19:35:09.670212Z","shell.execute_reply.started":"2024-04-13T19:35:09.518585Z","shell.execute_reply":"2024-04-13T19:35:09.669416Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 3. Define medsam, mask and bounding box functions","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.671277Z","iopub.execute_input":"2024-04-13T19:35:09.671563Z","iopub.status.idle":"2024-04-13T19:35:09.675750Z","shell.execute_reply.started":"2024-04-13T19:35:09.671539Z","shell.execute_reply":"2024-04-13T19:35:09.674821Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.676833Z","iopub.execute_input":"2024-04-13T19:35:09.677364Z","iopub.status.idle":"2024-04-13T19:35:09.688078Z","shell.execute_reply.started":"2024-04-13T19:35:09.677340Z","shell.execute_reply":"2024-04-13T19:35:09.687329Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    counter = 0\n    box_1024_batch = []\n    useless_image_indices = []\n    for ind, mask in enumerate(batch_mask):\n#         print(\"mask shape\", mask.shape)\n        mask = mask[0]\n#         print(\"mask shape 2\", mask.shape)\n        nonzero_indices = torch.nonzero(mask).T\n#         print(\"nonzero_indices shape:\", nonzero_indices.shape)\n#         nonzero_indices = torch.transpose(torch.nonzero(mask).T)\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n            box_1024 = torch.asarray([[-1, -1, -1, -1]])\n#             x0 = 0\n#             y0 = 0\n#             x1 = \n        else:\n            min_y = torch.min(nonzero_indices[0, :])\n            max_y = torch.max(nonzero_indices[0, :])\n            min_x = torch.min(nonzero_indices[1, :])\n            max_x = torch.max(nonzero_indices[1, :])\n\n            margin = 5\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n            # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n#         print(\"box 1024 shape\", box_1024.shape)\n        box_1024_batch.append(box_1024)\n\n    if counter == BATCH_SIZE:\n        return         \n        \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n\n#     print(final_tensor.shape)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.689224Z","iopub.execute_input":"2024-04-13T19:35:09.690011Z","iopub.status.idle":"2024-04-13T19:35:09.700348Z","shell.execute_reply.started":"2024-04-13T19:35:09.689976Z","shell.execute_reply":"2024-04-13T19:35:09.699634Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    # print(\"x_0, y_0\", x0, y0)\n    # print(\"w, h\", w, h)\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.701499Z","iopub.execute_input":"2024-04-13T19:35:09.702080Z","iopub.status.idle":"2024-04-13T19:35:09.713837Z","shell.execute_reply.started":"2024-04-13T19:35:09.702050Z","shell.execute_reply":"2024-04-13T19:35:09.713067Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def generate_box_and_embedding(img, img_mask):\n    H = 240\n    W = 240\n\n    #%% image preprocessing part 2\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img_1024 = resizer(img)\n#     print(\"img_1024 shape: \", img_1024.shape)\n\n    # generate boxes and useless image indices\n    box_1024, useless_image_indices = box_coordinates_from_mask_batch(img_mask)\n#     print(img_1024.shape)\n\n    # generate negated splicer\n    useful_indices = torch.ones(BATCH_SIZE, dtype=bool)\n    for index in useless_image_indices:\n        useful_indices[index] = False\n#     negated_splicer = ~splicer\n    \n#     img_1024 = img_1024.repeat(1, 3, 1, 1)\n    \n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(img_1024) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n    return image_embedding, box_1024, useful_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.717417Z","iopub.execute_input":"2024-04-13T19:35:09.718026Z","iopub.status.idle":"2024-04-13T19:35:09.727437Z","shell.execute_reply.started":"2024-04-13T19:35:09.718001Z","shell.execute_reply":"2024-04-13T19:35:09.726649Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test = torch.Tensor([1,2,3])\ntest[[False,False, True]]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.728402Z","iopub.execute_input":"2024-04-13T19:35:09.728675Z","iopub.status.idle":"2024-04-13T19:35:09.740430Z","shell.execute_reply.started":"2024-04-13T19:35:09.728652Z","shell.execute_reply":"2024-04-13T19:35:09.739599Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([3.])"},"metadata":{}}]},{"cell_type":"code","source":"# Medsam inference code\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n#     if len(box_torch.shape) == 2:\n#         box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    \n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.741423Z","iopub.execute_input":"2024-04-13T19:35:09.741764Z","iopub.status.idle":"2024-04-13T19:35:09.749562Z","shell.execute_reply.started":"2024-04-13T19:35:09.741732Z","shell.execute_reply":"2024-04-13T19:35:09.748676Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training loop","metadata":{}},{"cell_type":"code","source":"def all_false(lst):\n    return all(element == False for element in lst)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.750520Z","iopub.execute_input":"2024-04-13T19:35:09.750778Z","iopub.status.idle":"2024-04-13T19:35:09.762544Z","shell.execute_reply.started":"2024-04-13T19:35:09.750759Z","shell.execute_reply":"2024-04-13T19:35:09.761806Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(len(val_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.763559Z","iopub.execute_input":"2024-04-13T19:35:09.764462Z","iopub.status.idle":"2024-04-13T19:35:09.785566Z","shell.execute_reply.started":"2024-04-13T19:35:09.764433Z","shell.execute_reply":"2024-04-13T19:35:09.784714Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"28985\n","output_type":"stream"}]},{"cell_type":"code","source":"print(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:35:09.786622Z","iopub.execute_input":"2024-04-13T19:35:09.786907Z","iopub.status.idle":"2024-04-13T19:35:09.791345Z","shell.execute_reply.started":"2024-04-13T19:35:09.786870Z","shell.execute_reply":"2024-04-13T19:35:09.790431Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training loop\n# Initialize metrics and metrics lists\nfrom time import time\nstart_time = time()\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\n\nwith torch.no_grad():\n    for i, batch in enumerate(tqdm(val_dataloader)):\n        if i%100 == 99:\n            print(\"Iteration nb: \", i)\n            print(\"Elapsed time:\", time() - start_time)\n#         print(\"Iteration \", i)\n        # get image and masks from dataloader, change nb channels = 3\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'] #.to(device).float()\n        imgs = imgs.repeat(1, 3, 1, 1)\n#         img_masks = img_masks.repeat(1, 3, 1, 1)\n        # get non-empty image embeddings, bounding boxes and masks (i.e. labels)\n        image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n        if all_false(useful_indices):\n            continue\n        useful_box_1024 = box_1024[useful_indices,:,:][0]\n        useful_image_embedding = image_embedding[useful_indices,:,:,:][0]\n        y_true = img_masks[useful_indices,:,:,:][0]\n        useful_imgs = imgs[useful_indices,:,:,:][0]\n        \n#         print(\"image embedding shape\", useful_image_embedding.shape)\n#         print(\"box shape\", useful_box_1024.shape)\n#         print(\"mask shape\", y_true.shape)\n\n        # generate predictions using medsam_inference\n#         print(medsam_pred.shape)\n        output = medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W)\n        y_pred = torch.from_numpy(output).unsqueeze(dim=0)\n#         y_pred = net(imgs).cpu()\n        \n#         print(\"preds shape\", y_pred.shape)\n#         print(\"mask shape\", y_true.shape)\n        #TODO remove\n#         y_true = y_true[0]\n        \n        batch_dice_score = dice_metric(y_pred, y_true)\n#         print(\"Dice\", batch_dice_score)\n        valid_batch_dice.append(batch_dice_score)\n\n        batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n#         print(\"Jaccard\", batch_jaccard_score)\n        valid_batch_jaccard.append(batch_jaccard_score)\n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:39:23.136843Z","iopub.execute_input":"2024-04-13T19:39:23.137333Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28985 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d00e74b14b248919036fea860157f28"}},"metadata":{}},{"name":"stdout","text":"Iteration nb:  99\nElapsed time: 24.177688121795654\n","output_type":"stream"}]},{"cell_type":"code","source":"#%% visualize results\nbox_240 = useful_box_1024 / torch.asarray([1024, 1024, 1024, 1024]) * 240\nfig, ax = plt.subplots(1, 3, figsize=(10, 5))\nax[0].imshow(useful_imgs[0].cpu())\nshow_box(box_240[0].numpy(), ax[0])\nax[0].set_title(\"Input Image and Bounding Box\")\nax[1].imshow(y_pred[0])\n# show_mask(medsam_seg, ax[1])\n# show_box(box_np[0], ax[1])\nax[1].set_title(\"MedSAM Segmentation\")\n\nax[2].imshow(y_true[0])\nax[2].set_title(\"Mask\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:38:29.146397Z","iopub.status.idle":"2024-04-13T19:38:29.146940Z","shell.execute_reply.started":"2024-04-13T19:38:29.146629Z","shell.execute_reply":"2024-04-13T19:38:29.146701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:38:29.148254Z","iopub.status.idle":"2024-04-13T19:38:29.148730Z","shell.execute_reply.started":"2024-04-13T19:38:29.148500Z","shell.execute_reply":"2024-04-13T19:38:29.148519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:38:29.150081Z","iopub.status.idle":"2024-04-13T19:38:29.150429Z","shell.execute_reply.started":"2024-04-13T19:38:29.150269Z","shell.execute_reply":"2024-04-13T19:38:29.150284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Old code","metadata":{}},{"cell_type":"code","source":"# # Training loop\n# valid_batch_dice = []\n# valid_batch_jaccard = []\n# dice_metric = Dice()\n# jaccard_index_metric = BinaryJaccardIndex()\n# with torch.no_grad():\n#     H = 240\n#     W = 240\n#     for i, batch in enumerate(tqdm(val_dataloader)):\n#         # get image and masks from dataloader\n#         imgs = batch['image'].to(device).float()\n#         img_masks = batch['mask'] #.to(device).float()\n#         imgs = imgs.repeat(1, 3, 1, 1)\n#         img_masks = img_masks.repeat(1, 3, 1, 1)\n        \n#         # get useful image embeddings, bounding boxes and masks = labels\n#         image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n#         useful_box_1024 = box_1024[useful_indices,:,:]\n#         useful_image_embedding = image_embedding[useful_indices,:,:,:]\n#         y_true = img_masks[useful_indices,:,:,:]\n        \n#         print(\"image embedding shape\", useful_image_embedding.shape)\n#         print(\"box shape\", useful_box_1024.shape)\n#         print(\"mask shape\", useful_img_masks.shape)\n\n#         y_pred = torch.from_numpy(medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W))\n# #         y_pred = net(imgs).cpu()\n        \n#         print(\"preds shape\", y_pred.shape)\n#         print(\"mask shape\", y_true.shape)\n#         #TODO remove\n#         y_true = y_true[0][0]\n        \n#         batch_dice_score = dice_metric(y_pred, y_true)\n#         print(\"Dice\", batch_dice_score)\n#         valid_batch_dice.append(batch_dice_score)\n\n#         batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n#         valid_batch_jaccard.append(batch_jaccard_score)\n\n# #         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:38:29.151668Z","iopub.status.idle":"2024-04-13T19:38:29.152066Z","shell.execute_reply.started":"2024-04-13T19:38:29.151869Z","shell.execute_reply":"2024-04-13T19:38:29.151884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_box_and_embedding(img_np, mask_np):\n# #     slice_indexes.append(slice_ind)\n# #     if len(img_np.shape) == 2:\n# #         img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n# #     else:\n# #         img_3c = img_np\n# #     H, W, _ = img_3c.shape\n#     H = 240\n#     W = 240\n\n#     #%% image preprocessing part 2\n#     # 16,1,1024,1024\n#     resizer = torchvision.transforms.Resize(size = (1024,1024))\n#     img_1024 = resizer(img_np)\n# #     img_1024 = transform.resize(img_np, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n#     print(\"img_1024 shape: \", img_1024.shape)\n# #     img_1024 = (img_1024 - img_1024.min()) / np.clip(\n# #         img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n# #     )  # normalize to [0, 1], (H, W, 3)\n# #     # convert the shape to (3, H, W)\n# #     img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n#     # need to rewrite box coordinates from mask as input is now [16,1,240,240]\n# #     x0, y0, x1, y1 = box_coordinates_from_mask(mask_np)\n# #     box_np = np.array([[x0,y0, x1, y1]])\n# #     box_1024 = box_np / np.array([W, H, W, H]) * 1024\n#     box_1024, useless_image_indices = box_coordinates_from_mask_batch(mask_np)\n#     print(img_1024.shape)\n    \n#     mask = torch.ones(BATCH_SIZE, dtype=bool)\n#     for index in useless_image_indices:\n#         mask[index] = False\n#     # Negate the mask\n#     negated_mask = ~mask\n# #     print(\"negated mask\", negated_mask)\n# #     print(\"img_1024.shape\", img_1024.shape)\n#     filtered_img_1024_tensor = img_1024[negated_mask,:,:,:]    \n    \n#     filtered_img_1024_tensor_repeated = filtered_img_1024_tensor.repeat(1, 3, 1, 1)\n#     print(\"filtered shape\", filtered_img_1024_tensor.shape)\n#     with torch.no_grad():\n#         image_embedding = medsam_model.image_encoder(filtered_img_1024_tensor_repeated) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n#     return image_embedding, box_1024, negated_mask","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:38:29.153379Z","iopub.status.idle":"2024-04-13T19:38:29.153692Z","shell.execute_reply.started":"2024-04-13T19:38:29.153533Z","shell.execute_reply":"2024-04-13T19:38:29.153546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:38:29.155287Z","iopub.status.idle":"2024-04-13T19:38:29.155590Z","shell.execute_reply.started":"2024-04-13T19:38:29.155440Z","shell.execute_reply":"2024-04-13T19:38:29.155453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}