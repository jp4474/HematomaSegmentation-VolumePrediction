{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:24.004856Z","iopub.execute_input":"2024-04-13T18:11:24.005726Z","iopub.status.idle":"2024-04-13T18:11:24.013338Z","shell.execute_reply.started":"2024-04-13T18:11:24.005693Z","shell.execute_reply":"2024-04-13T18:11:24.012217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:31.242074Z","iopub.execute_input":"2024-04-13T18:11:31.242782Z","iopub.status.idle":"2024-04-13T18:11:31.250651Z","shell.execute_reply.started":"2024-04-13T18:11:31.242750Z","shell.execute_reply":"2024-04-13T18:11:31.249667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:34.800831Z","iopub.execute_input":"2024-04-13T18:11:34.801530Z","iopub.status.idle":"2024-04-13T18:11:34.806110Z","shell.execute_reply.started":"2024-04-13T18:11:34.801496Z","shell.execute_reply":"2024-04-13T18:11:34.805227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:36.793496Z","iopub.execute_input":"2024-04-13T18:11:36.794317Z","iopub.status.idle":"2024-04-13T18:11:36.798510Z","shell.execute_reply.started":"2024-04-13T18:11:36.794285Z","shell.execute_reply":"2024-04-13T18:11:36.797484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:38.746655Z","iopub.execute_input":"2024-04-13T18:11:38.747303Z","iopub.status.idle":"2024-04-13T18:11:38.891926Z","shell.execute_reply.started":"2024-04-13T18:11:38.747269Z","shell.execute_reply":"2024-04-13T18:11:38.891101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load MedSAM","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:45.115316Z","iopub.execute_input":"2024-04-13T18:11:45.116002Z","iopub.status.idle":"2024-04-13T18:11:45.121187Z","shell.execute_reply.started":"2024-04-13T18:11:45.115967Z","shell.execute_reply":"2024-04-13T18:11:45.120163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:47.025865Z","iopub.execute_input":"2024-04-13T18:11:47.026538Z","iopub.status.idle":"2024-04-13T18:11:47.035119Z","shell.execute_reply.started":"2024-04-13T18:11:47.026502Z","shell.execute_reply":"2024-04-13T18:11:47.033949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:49.028155Z","iopub.execute_input":"2024-04-13T18:11:49.029056Z","iopub.status.idle":"2024-04-13T18:11:49.036287Z","shell.execute_reply.started":"2024-04-13T18:11:49.029021Z","shell.execute_reply":"2024-04-13T18:11:49.035017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    counter = 0\n    W = 240\n    H = 240\n    box_1024_batch = []\n    useless_image_indices = []\n    for ind, mask in enumerate(batch_mask):\n#         print(\"mask shape\", mask.shape)\n        mask = mask[0]\n#         print(\"mask shape 2\", mask.shape)\n        nonzero_indices = torch.nonzero(mask).T\n#         print(\"nonzero_indices shape:\", nonzero_indices.shape)\n#         nonzero_indices = torch.transpose(torch.nonzero(mask).T)\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n            box_1024 = torch.asarray([[-1, -1, -1, -1]])\n#             x0 = 0\n#             y0 = 0\n#             x1 = \n        else:\n            min_y = torch.min(nonzero_indices[0, :])\n            max_y = torch.max(nonzero_indices[0, :])\n            min_x = torch.min(nonzero_indices[1, :])\n            max_x = torch.max(nonzero_indices[1, :])\n\n            margin = 5\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n            # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n        print(\"box 1024 shape\", box_1024.shape)\n        box_1024_batch.append(box_1024)\n\n    if counter == BATCH_SIZE:\n        return         \n        \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n\n    print(final_tensor.shape)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:13:16.946525Z","iopub.execute_input":"2024-04-13T18:13:16.946917Z","iopub.status.idle":"2024-04-13T18:13:16.958317Z","shell.execute_reply.started":"2024-04-13T18:13:16.946876Z","shell.execute_reply":"2024-04-13T18:13:16.957302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    # print(\"x_0, y_0\", x0, y0)\n    # print(\"w, h\", w, h)\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:53.524201Z","iopub.execute_input":"2024-04-13T18:11:53.524968Z","iopub.status.idle":"2024-04-13T18:11:53.536805Z","shell.execute_reply.started":"2024-04-13T18:11:53.524933Z","shell.execute_reply":"2024-04-13T18:11:53.535802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda:0\"\nmedsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = medsam_model.to(device)\nmedsam_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:11:56.250820Z","iopub.execute_input":"2024-04-13T18:11:56.251684Z","iopub.status.idle":"2024-04-13T18:11:57.603384Z","shell.execute_reply.started":"2024-04-13T18:11:56.251652Z","shell.execute_reply":"2024-04-13T18:11:57.602549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_box_and_embedding(img, img_mask):\n    H = 240\n    W = 240\n\n    #%% image preprocessing part 2\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img_1024 = resizer(img)\n    print(\"img_1024 shape: \", img_1024.shape)\n\n    # generate boxes and useless image indices\n    box_1024, useless_image_indices = box_coordinates_from_mask_batch(img_mask)\n    print(img_1024.shape)\n\n    # generate negated splicer\n    splicer = torch.ones(BATCH_SIZE, dtype=bool)\n    for index in useless_image_indices:\n        splicer[index] = False\n    negated_splicer = ~splicer\n    \n    img_1024 = img_1024.repeat(1, 3, 1, 1)\n    \n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(img_1024) # (1, 256, 64, 64)\n    print(\"image_embedding shape\", image_embedding.shape)\n    print(\"box_1024 shape\", box_1024.shape)\n    return image_embedding, box_1024, negated_splicer","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:21:32.821310Z","iopub.execute_input":"2024-04-13T18:21:32.822230Z","iopub.status.idle":"2024-04-13T18:21:32.829715Z","shell.execute_reply.started":"2024-04-13T18:21:32.822197Z","shell.execute_reply":"2024-04-13T18:21:32.828631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Medsam inference code\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n    if len(box_torch.shape) == 2:\n        box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:21:35.764311Z","iopub.execute_input":"2024-04-13T18:21:35.764652Z","iopub.status.idle":"2024-04-13T18:21:35.773020Z","shell.execute_reply.started":"2024-04-13T18:21:35.764625Z","shell.execute_reply":"2024-04-13T18:21:35.772069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\nwith torch.no_grad():\n    H = 240\n    W = 240\n    for i, batch in enumerate(tqdm(validation_dataloader)):\n        # get image and masks from dataloader\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'] #.to(device).float()\n        \n        # get useful image embeddings, bounding boxes and masks = labels\n        image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, true_masks)\n        useful_box_1024 = box_1024[useful_indices,:,:]\n        useful_image_embedding = image_embedding[useful_indices,:,:,:]\n        useful_img_masks = img_masks[useful_indices,:,:,:]\n        \n        print(\"image embedding shape\", useful_image_embedding.shape)\n        print(\"box shape\", useful_box_1024.shape)\n        print(\"mask shape\", useful_img_masks.shape)\n\n        y_pred = torch.from_numpy(medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W)).unsqueeze(dim = 1)\n#         y_pred = net(imgs).cpu()\n        \n        print(\"preds shape\", y_pred.shape)\n        print(\"mask shape\", filtered_true_masks.shape)\n        \n        batch_dice_score = dice_metric(y_pred, filtered_true_masks)\n        print(\"Dice\", batch_dice_score)\n        valid_batch_dice.append(batch_dice_score)\n\n        batch_jaccard_score = jaccard_index_metric(y_pred, filtered_true_masks)\n        valid_batch_jaccard.append(batch_jaccard_score)\n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:24:08.568049Z","iopub.execute_input":"2024-04-13T18:24:08.569099Z","iopub.status.idle":"2024-04-13T18:24:09.775943Z","shell.execute_reply.started":"2024-04-13T18:24:08.569061Z","shell.execute_reply":"2024-04-13T18:24:09.774499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, batch in enumerate(validation_dataloader):\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard = train_net(ddp_model, EPOCHS, train_dataloader, validation_dataloader, optimizer, loss_function)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Old code","metadata":{}},{"cell_type":"code","source":"# def generate_box_and_embedding(img_np, mask_np):\n# #     slice_indexes.append(slice_ind)\n# #     if len(img_np.shape) == 2:\n# #         img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n# #     else:\n# #         img_3c = img_np\n# #     H, W, _ = img_3c.shape\n#     H = 240\n#     W = 240\n\n#     #%% image preprocessing part 2\n#     # 16,1,1024,1024\n#     resizer = torchvision.transforms.Resize(size = (1024,1024))\n#     img_1024 = resizer(img_np)\n# #     img_1024 = transform.resize(img_np, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n#     print(\"img_1024 shape: \", img_1024.shape)\n# #     img_1024 = (img_1024 - img_1024.min()) / np.clip(\n# #         img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n# #     )  # normalize to [0, 1], (H, W, 3)\n# #     # convert the shape to (3, H, W)\n# #     img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n#     # need to rewrite box coordinates from mask as input is now [16,1,240,240]\n# #     x0, y0, x1, y1 = box_coordinates_from_mask(mask_np)\n# #     box_np = np.array([[x0,y0, x1, y1]])\n# #     box_1024 = box_np / np.array([W, H, W, H]) * 1024\n#     box_1024, useless_image_indices = box_coordinates_from_mask_batch(mask_np)\n#     print(img_1024.shape)\n    \n#     mask = torch.ones(BATCH_SIZE, dtype=bool)\n#     for index in useless_image_indices:\n#         mask[index] = False\n#     # Negate the mask\n#     negated_mask = ~mask\n# #     print(\"negated mask\", negated_mask)\n# #     print(\"img_1024.shape\", img_1024.shape)\n#     filtered_img_1024_tensor = img_1024[negated_mask,:,:,:]    \n    \n#     filtered_img_1024_tensor_repeated = filtered_img_1024_tensor.repeat(1, 3, 1, 1)\n#     print(\"filtered shape\", filtered_img_1024_tensor.shape)\n#     with torch.no_grad():\n#         image_embedding = medsam_model.image_encoder(filtered_img_1024_tensor_repeated) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n#     return image_embedding, box_1024, negated_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}