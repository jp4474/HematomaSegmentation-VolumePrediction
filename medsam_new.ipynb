{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:39.282354Z","iopub.execute_input":"2024-04-13T18:32:39.282636Z","iopub.status.idle":"2024-04-13T18:32:45.554272Z","shell.execute_reply.started":"2024-04-13T18:32:39.282612Z","shell.execute_reply":"2024-04-13T18:32:45.553237Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:45.561425Z","iopub.execute_input":"2024-04-13T18:32:45.561715Z","iopub.status.idle":"2024-04-13T18:32:45.565643Z","shell.execute_reply.started":"2024-04-13T18:32:45.561690Z","shell.execute_reply":"2024-04-13T18:32:45.564636Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:45.566918Z","iopub.execute_input":"2024-04-13T18:32:45.567576Z","iopub.status.idle":"2024-04-13T18:32:45.578655Z","shell.execute_reply.started":"2024-04-13T18:32:45.567550Z","shell.execute_reply":"2024-04-13T18:32:45.577732Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:45.580983Z","iopub.execute_input":"2024-04-13T18:32:45.581256Z","iopub.status.idle":"2024-04-13T18:32:45.589526Z","shell.execute_reply.started":"2024-04-13T18:32:45.581232Z","shell.execute_reply":"2024-04-13T18:32:45.588670Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:08.756173Z","iopub.execute_input":"2024-04-13T18:39:08.756558Z","iopub.status.idle":"2024-04-13T18:39:08.761274Z","shell.execute_reply.started":"2024-04-13T18:39:08.756525Z","shell.execute_reply":"2024-04-13T18:39:08.760278Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:11.709728Z","iopub.execute_input":"2024-04-13T18:39:11.710081Z","iopub.status.idle":"2024-04-13T18:39:11.861294Z","shell.execute_reply.started":"2024-04-13T18:39:11.710053Z","shell.execute_reply":"2024-04-13T18:39:11.860307Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Load MedSAM","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:14.234421Z","iopub.execute_input":"2024-04-13T18:39:14.234798Z","iopub.status.idle":"2024-04-13T18:39:14.240103Z","shell.execute_reply.started":"2024-04-13T18:39:14.234767Z","shell.execute_reply":"2024-04-13T18:39:14.238800Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:15.307064Z","iopub.execute_input":"2024-04-13T18:39:15.307407Z","iopub.status.idle":"2024-04-13T18:39:15.316433Z","shell.execute_reply.started":"2024-04-13T18:39:15.307381Z","shell.execute_reply":"2024-04-13T18:39:15.315376Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:18.217748Z","iopub.execute_input":"2024-04-13T18:39:18.218146Z","iopub.status.idle":"2024-04-13T18:39:18.225350Z","shell.execute_reply.started":"2024-04-13T18:39:18.218111Z","shell.execute_reply":"2024-04-13T18:39:18.224340Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    counter = 0\n    W = 240\n    H = 240\n    box_1024_batch = []\n    useless_image_indices = []\n    for ind, mask in enumerate(batch_mask):\n#         print(\"mask shape\", mask.shape)\n        mask = mask[0]\n#         print(\"mask shape 2\", mask.shape)\n        nonzero_indices = torch.nonzero(mask).T\n#         print(\"nonzero_indices shape:\", nonzero_indices.shape)\n#         nonzero_indices = torch.transpose(torch.nonzero(mask).T)\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n            box_1024 = torch.asarray([[-1, -1, -1, -1]])\n#             x0 = 0\n#             y0 = 0\n#             x1 = \n        else:\n            min_y = torch.min(nonzero_indices[0, :])\n            max_y = torch.max(nonzero_indices[0, :])\n            min_x = torch.min(nonzero_indices[1, :])\n            max_x = torch.max(nonzero_indices[1, :])\n\n            margin = 5\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n            # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n        print(\"box 1024 shape\", box_1024.shape)\n        box_1024_batch.append(box_1024)\n\n    if counter == BATCH_SIZE:\n        return         \n        \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n\n    print(final_tensor.shape)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:20.468094Z","iopub.execute_input":"2024-04-13T18:39:20.468482Z","iopub.status.idle":"2024-04-13T18:39:20.479127Z","shell.execute_reply.started":"2024-04-13T18:39:20.468451Z","shell.execute_reply":"2024-04-13T18:39:20.478160Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    # print(\"x_0, y_0\", x0, y0)\n    # print(\"w, h\", w, h)\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:22.755442Z","iopub.execute_input":"2024-04-13T18:39:22.755817Z","iopub.status.idle":"2024-04-13T18:39:22.765742Z","shell.execute_reply.started":"2024-04-13T18:39:22.755785Z","shell.execute_reply":"2024-04-13T18:39:22.764804Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda:0\"\nmedsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = medsam_model.to(device)\nmedsam_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:25.444688Z","iopub.execute_input":"2024-04-13T18:39:25.445098Z","iopub.status.idle":"2024-04-13T18:39:26.820858Z","shell.execute_reply.started":"2024-04-13T18:39:25.445064Z","shell.execute_reply":"2024-04-13T18:39:26.819864Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def generate_box_and_embedding(img, img_mask):\n    H = 240\n    W = 240\n\n    #%% image preprocessing part 2\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img_1024 = resizer(img)\n    print(\"img_1024 shape: \", img_1024.shape)\n\n    # generate boxes and useless image indices\n    box_1024, useless_image_indices = box_coordinates_from_mask_batch(img_mask)\n    print(img_1024.shape)\n\n    # generate negated splicer\n    splicer = torch.ones(BATCH_SIZE, dtype=bool)\n    for index in useless_image_indices:\n        splicer[index] = False\n    negated_splicer = ~splicer\n    \n#     img_1024 = img_1024.repeat(1, 3, 1, 1)\n    \n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(img_1024) # (1, 256, 64, 64)\n    print(\"image_embedding shape\", image_embedding.shape)\n    print(\"box_1024 shape\", box_1024.shape)\n    return image_embedding, box_1024, negated_splicer","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:47:02.359036Z","iopub.execute_input":"2024-04-13T18:47:02.359941Z","iopub.status.idle":"2024-04-13T18:47:02.367124Z","shell.execute_reply.started":"2024-04-13T18:47:02.359885Z","shell.execute_reply":"2024-04-13T18:47:02.366173Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Medsam inference code\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n#     if len(box_torch.shape) == 2:\n#         box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    \n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:39:31.548055Z","iopub.execute_input":"2024-04-13T18:39:31.548929Z","iopub.status.idle":"2024-04-13T18:39:31.559161Z","shell.execute_reply.started":"2024-04-13T18:39:31.548872Z","shell.execute_reply":"2024-04-13T18:39:31.558191Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Training loop\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\nwith torch.no_grad():\n    H = 240\n    W = 240\n    for i, batch in enumerate(tqdm(val_dataloader)):\n        # get image and masks from dataloader\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'] #.to(device).float()\n        imgs = imgs.repeat(1, 3, 1, 1)\n        img_masks = img_masks.repeat(1, 3, 1, 1)\n        \n        # get useful image embeddings, bounding boxes and masks = labels\n        image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n        useful_box_1024 = box_1024[useful_indices,:,:]\n        useful_image_embedding = image_embedding[useful_indices,:,:,:]\n        y_true = img_masks[useful_indices,:,:,:]\n        \n        print(\"image embedding shape\", useful_image_embedding.shape)\n        print(\"box shape\", useful_box_1024.shape)\n        print(\"mask shape\", useful_img_masks.shape)\n\n        y_pred = torch.from_numpy(medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W))\n#         y_pred = net(imgs).cpu()\n        \n        print(\"preds shape\", y_pred.shape)\n        print(\"mask shape\", y_true.shape)\n        #TODO remove\n        y_true = y_true[0][0]\n        \n        batch_dice_score = dice_metric(y_pred, y_true)\n        print(\"Dice\", batch_dice_score)\n        valid_batch_dice.append(batch_dice_score)\n\n        batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n        valid_batch_jaccard.append(batch_jaccard_score)\n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:47:06.211909Z","iopub.execute_input":"2024-04-13T18:47:06.212594Z","iopub.status.idle":"2024-04-13T18:47:07.910791Z","shell.execute_reply.started":"2024-04-13T18:47:06.212558Z","shell.execute_reply":"2024-04-13T18:47:07.909280Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14493 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b70d28bccc848ef807d500dcdb1e2f2"}},"metadata":{}},{"name":"stdout","text":"img_1024 shape:  torch.Size([2, 3, 1024, 1024])\nbox 1024 shape torch.Size([1, 4])\nbox 1024 shape torch.Size([1, 4])\ntorch.Size([2, 1, 4])\ntorch.Size([2, 3, 1024, 1024])\nimage_embedding shape torch.Size([2, 256, 64, 64])\nbox_1024 shape torch.Size([2, 1, 4])\nimage embedding shape torch.Size([1, 256, 64, 64])\nbox shape torch.Size([1, 1, 4])\nmask shape torch.Size([1, 1, 240, 240])\npreds shape torch.Size([240, 240])\nmask shape torch.Size([1, 3, 240, 240])\nDice tensor(0.9563)\nimg_1024 shape:  torch.Size([2, 3, 1024, 1024])\nbox 1024 shape torch.Size([1, 4])\nbox 1024 shape torch.Size([1, 4])\ntorch.Size([2, 1, 4])\ntorch.Size([2, 3, 1024, 1024])\nimage_embedding shape torch.Size([2, 256, 64, 64])\nbox_1024 shape torch.Size([2, 1, 4])\nimage embedding shape torch.Size([1, 256, 64, 64])\nbox shape torch.Size([1, 1, 4])\nmask shape torch.Size([1, 1, 240, 240])\npreds shape torch.Size([240, 240])\nmask shape torch.Size([1, 3, 240, 240])\nDice tensor(0.9477)\nimg_1024 shape:  torch.Size([2, 3, 1024, 1024])\nbox 1024 shape torch.Size([1, 4])\nbox 1024 shape torch.Size([1, 4])\ntorch.Size([2, 1, 4])\ntorch.Size([2, 3, 1024, 1024])\nimage_embedding shape torch.Size([2, 256, 64, 64])\nbox_1024 shape torch.Size([2, 1, 4])\nimage embedding shape torch.Size([2, 256, 64, 64])\nbox shape torch.Size([2, 1, 4])\nmask shape torch.Size([1, 1, 240, 240])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, useful_box_1024\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, useful_img_masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 26\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mmedsam_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmedsam_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museful_image_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museful_box_1024\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         y_pred = net(imgs).cpu()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mmedsam_inference\u001b[0;34m(medsam_model, img_embed, box_1024, H, W)\u001b[0m\n\u001b[1;32m      6\u001b[0m     box_torch \u001b[38;5;241m=\u001b[39m box_torch[:, \u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;66;03m# (B, 1, 4)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m medsam_model\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[1;32m      9\u001b[0m     points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mbox_torch,\n\u001b[1;32m     11\u001b[0m     masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m low_res_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmedsam_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmedsam_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dense_pe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (1, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (B, 2, 256)\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m low_res_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(low_res_logits)  \u001b[38;5;66;03m# (1, 1, 256, 256)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m low_res_pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     24\u001b[0m     low_res_pred,\n\u001b[1;32m     25\u001b[0m     size\u001b[38;5;241m=\u001b[39m(H, W),\n\u001b[1;32m     26\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m )  \u001b[38;5;66;03m# (1, 1, gt.shape)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/mask_decoder.py:94\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     73\u001b[0m     image_embeddings: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     multimask_output: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     78\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m      torch.Tensor: batched predictions of mask quality\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     masks, iou_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Select the correct mask or masks for output\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multimask_output:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/mask_decoder.py:127\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Expand per-image data in batch direction to be per-mask\u001b[39;00m\n\u001b[1;32m    126\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_embeddings, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\n\u001b[1;32m    128\u001b[0m pos_src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_pe, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    129\u001b[0m b, c, h, w \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"],"ename":"RuntimeError","evalue":"The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0","output_type":"error"}]},{"cell_type":"code","source":"for i, batch in enumerate(validation_dataloader):\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:47.790408Z","iopub.status.idle":"2024-04-13T18:32:47.790804Z","shell.execute_reply.started":"2024-04-13T18:32:47.790625Z","shell.execute_reply":"2024-04-13T18:32:47.790641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard = train_net(ddp_model, EPOCHS, train_dataloader, validation_dataloader, optimizer, loss_function)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:47.791878Z","iopub.status.idle":"2024-04-13T18:32:47.792251Z","shell.execute_reply.started":"2024-04-13T18:32:47.792064Z","shell.execute_reply":"2024-04-13T18:32:47.792078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Old code","metadata":{}},{"cell_type":"code","source":"# def generate_box_and_embedding(img_np, mask_np):\n# #     slice_indexes.append(slice_ind)\n# #     if len(img_np.shape) == 2:\n# #         img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n# #     else:\n# #         img_3c = img_np\n# #     H, W, _ = img_3c.shape\n#     H = 240\n#     W = 240\n\n#     #%% image preprocessing part 2\n#     # 16,1,1024,1024\n#     resizer = torchvision.transforms.Resize(size = (1024,1024))\n#     img_1024 = resizer(img_np)\n# #     img_1024 = transform.resize(img_np, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n#     print(\"img_1024 shape: \", img_1024.shape)\n# #     img_1024 = (img_1024 - img_1024.min()) / np.clip(\n# #         img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n# #     )  # normalize to [0, 1], (H, W, 3)\n# #     # convert the shape to (3, H, W)\n# #     img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n#     # need to rewrite box coordinates from mask as input is now [16,1,240,240]\n# #     x0, y0, x1, y1 = box_coordinates_from_mask(mask_np)\n# #     box_np = np.array([[x0,y0, x1, y1]])\n# #     box_1024 = box_np / np.array([W, H, W, H]) * 1024\n#     box_1024, useless_image_indices = box_coordinates_from_mask_batch(mask_np)\n#     print(img_1024.shape)\n    \n#     mask = torch.ones(BATCH_SIZE, dtype=bool)\n#     for index in useless_image_indices:\n#         mask[index] = False\n#     # Negate the mask\n#     negated_mask = ~mask\n# #     print(\"negated mask\", negated_mask)\n# #     print(\"img_1024.shape\", img_1024.shape)\n#     filtered_img_1024_tensor = img_1024[negated_mask,:,:,:]    \n    \n#     filtered_img_1024_tensor_repeated = filtered_img_1024_tensor.repeat(1, 3, 1, 1)\n#     print(\"filtered shape\", filtered_img_1024_tensor.shape)\n#     with torch.no_grad():\n#         image_embedding = medsam_model.image_encoder(filtered_img_1024_tensor_repeated) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n#     return image_embedding, box_1024, negated_mask","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:32:47.793824Z","iopub.status.idle":"2024-04-13T18:32:47.794183Z","shell.execute_reply.started":"2024-04-13T18:32:47.794018Z","shell.execute_reply":"2024-04-13T18:32:47.794033Z"},"trusted":true},"execution_count":null,"outputs":[]}]}