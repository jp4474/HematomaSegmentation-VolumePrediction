{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:43:27.780615Z","iopub.execute_input":"2024-04-13T19:43:27.780968Z","iopub.status.idle":"2024-04-13T19:43:34.028184Z","shell.execute_reply.started":"2024-04-13T19:43:27.780938Z","shell.execute_reply":"2024-04-13T19:43:34.027215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:43:34.030568Z","iopub.execute_input":"2024-04-13T19:43:34.031176Z","iopub.status.idle":"2024-04-13T19:43:34.035918Z","shell.execute_reply.started":"2024-04-13T19:43:34.031141Z","shell.execute_reply":"2024-04-13T19:43:34.034801Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:43:34.037203Z","iopub.execute_input":"2024-04-13T19:43:34.037568Z","iopub.status.idle":"2024-04-13T19:43:34.047540Z","shell.execute_reply.started":"2024-04-13T19:43:34.037528Z","shell.execute_reply":"2024-04-13T19:43:34.046363Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:43:34.048809Z","iopub.execute_input":"2024-04-13T19:43:34.049085Z","iopub.status.idle":"2024-04-13T19:43:34.057891Z","shell.execute_reply.started":"2024-04-13T19:43:34.049060Z","shell.execute_reply":"2024-04-13T19:43:34.056995Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:13.891851Z","iopub.execute_input":"2024-04-13T19:45:13.892839Z","iopub.status.idle":"2024-04-13T19:45:13.897378Z","shell.execute_reply.started":"2024-04-13T19:45:13.892796Z","shell.execute_reply":"2024-04-13T19:45:13.896343Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:13.899656Z","iopub.execute_input":"2024-04-13T19:45:13.899996Z","iopub.status.idle":"2024-04-13T19:45:14.048407Z","shell.execute_reply.started":"2024-04-13T19:45:13.899963Z","shell.execute_reply":"2024-04-13T19:45:14.047602Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Load MedSAM","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:14.049757Z","iopub.execute_input":"2024-04-13T19:45:14.050049Z","iopub.status.idle":"2024-04-13T19:45:14.054430Z","shell.execute_reply.started":"2024-04-13T19:45:14.050023Z","shell.execute_reply":"2024-04-13T19:45:14.053500Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:14.055443Z","iopub.execute_input":"2024-04-13T19:45:14.055748Z","iopub.status.idle":"2024-04-13T19:45:14.065731Z","shell.execute_reply.started":"2024-04-13T19:45:14.055724Z","shell.execute_reply":"2024-04-13T19:45:14.064968Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:14.068682Z","iopub.execute_input":"2024-04-13T19:45:14.069767Z","iopub.status.idle":"2024-04-13T19:45:14.079409Z","shell.execute_reply.started":"2024-04-13T19:45:14.069739Z","shell.execute_reply":"2024-04-13T19:45:14.078745Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    W = 240\n    H = 240\n    margin = 5\n    box_1024_batch = []\n    useless_image_indices = []\n    \n    for ind, mask in enumerate(batch_mask):\n        mask = mask[0]\n        nonzero_indices = torch.nonzero(mask).T\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n#             #box_1024 = torch.asarray([[-1, -1, -1, -1]])\n        else:\n            min_y = torch.min(nonzero_indices[0, :]).values\n            max_y = torch.max(nonzero_indices[0, :]).values\n            min_x = torch.min(nonzero_indices[1, :]).values\n            max_x = torch.max(nonzero_indices[1, :]).values\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n            box_1024_batch.append(box_1024)\n    \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:14.080331Z","iopub.execute_input":"2024-04-13T19:45:14.080614Z","iopub.status.idle":"2024-04-13T19:45:14.091493Z","shell.execute_reply.started":"2024-04-13T19:45:14.080562Z","shell.execute_reply":"2024-04-13T19:45:14.090722Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:14.092506Z","iopub.execute_input":"2024-04-13T19:45:14.092788Z","iopub.status.idle":"2024-04-13T19:45:14.105120Z","shell.execute_reply.started":"2024-04-13T19:45:14.092765Z","shell.execute_reply":"2024-04-13T19:45:14.104185Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda:0\"\nmedsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = medsam_model.to(device)\nmedsam_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:45:14.106172Z","iopub.execute_input":"2024-04-13T19:45:14.106441Z","iopub.status.idle":"2024-04-13T19:45:15.466516Z","shell.execute_reply.started":"2024-04-13T19:45:14.106418Z","shell.execute_reply":"2024-04-13T19:45:15.465618Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5, W = 240, H = 240):\n    box_1024_batch = []\n    \n    for ind, mask in enumerate(batch_mask):\n        mask = mask[0]\n        nonzero_indices = torch.nonzero(mask).T\n        \n        #if(nonzero_indices.shape[-1] == 0):\n        min_y = torch.min(nonzero_indices[0, :])\n        max_y = torch.max(nonzero_indices[0, :])\n        min_x = torch.min(nonzero_indices[1, :])\n        max_x = torch.max(nonzero_indices[1, :])\n\n        x0 = min_x - margin\n        y0 = min_y - margin\n        x1 = max_x + margin\n        y1 = max_y + margin\n\n        box = torch.asarray([[x0,y0, x1, y1]])\n        box = box / torch.asarray([W, H, W, H]) * 1024\n        box_1024_batch.append(box)\n    \n    box_batch = torch.cat(box_1024_batch, dim = 0)\n    box_batch = box_batch.unsqueeze(dim = 1)\n    return box_batch\n\ndef generate_box_and_embedding(img, mask, W = 240, H = 240):\n\n    # resize image, make it 3D\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img = resizer(img)\n    img = img.repeat(1, 3, 1, 1)\n    \n    #mask = resizer(mask)\n    # generate bounding boxes\n    box_batch = box_coordinates_from_mask_batch(mask)\n  \n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(img) # (1, 256, 64, 64)\n        \n    return image_embedding, mask, box_batch","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:47:58.923388Z","iopub.execute_input":"2024-04-13T19:47:58.923781Z","iopub.status.idle":"2024-04-13T19:47:58.934122Z","shell.execute_reply.started":"2024-04-13T19:47:58.923747Z","shell.execute_reply":"2024-04-13T19:47:58.933202Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Medsam inference code\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n    if len(box_torch.shape) == 2:\n        box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    \n#     print(f'img_embed: {img_embed.shape} should be (B, 256, 64, 64)')\n#     print(f'sparse_embeddings : {sparse_embeddings.shape} (B, 2, 256)')\n#     print(f'dense_prompt_embeddings : {dense_embeddings.shape} (B, 256, 64, 64)')\n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:47:58.936061Z","iopub.execute_input":"2024-04-13T19:47:58.936333Z","iopub.status.idle":"2024-04-13T19:47:58.951258Z","shell.execute_reply.started":"2024-04-13T19:47:58.936309Z","shell.execute_reply":"2024-04-13T19:47:58.950468Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"valid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice().to(device)\njaccard_index_metric = BinaryJaccardIndex().to(device)\nwith torch.no_grad():\n    H = 240\n    W = 240\n    for i, batch in enumerate(tqdm(test_dataloader)):\n        # get image and masks from dataloader\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'].to(device).float()\n        \n        indices_to_remove = []\n        \n        for idx, img_mask in enumerate(img_masks):\n            if (torch.sum(img_mask) == 0):\n                indices_to_remove.append(idx)\n        \n        if(len(indices_to_remove) != BATCH_SIZE):\n            indices_to_select = set(range(0, BATCH_SIZE)).difference(set(indices_to_remove))\n            indices_to_select = list(indices_to_select)\n\n            imgs = imgs[indices_to_select,:,:,:]\n            img_masks = img_masks[indices_to_select,:,:,:]\n\n            image_embedding, true_mask, box = generate_box_and_embedding(imgs, img_masks)\n             \n            true_mask = true_mask.int()\n            true_mask = true_mask[0][0]\n            y_pred = medsam_inference(medsam_model, image_embedding, box, H, W)\n            y_pred = torch.from_numpy(y_pred)\n            #y_pred = y_pred.unsqueeze(dim = 1)\n            \n            y_pred = y_pred.to(device)\n            true_mask = true_mask.to(device)\n\n            batch_dice_score = dice_metric(y_pred, true_mask)\n            valid_batch_dice.append(batch_dice_score)\n\n            batch_jaccard_score = jaccard_index_metric(y_pred, true_mask)\n            valid_batch_jaccard.append(batch_jaccard_score)\n\n            print(f'DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')\n    print(f'Validation DICE score: {np.mean(valid_batch_dice)}, Validation Jaccard score: {np.mean(jaccard_index_metric)}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:52:09.180518Z","iopub.execute_input":"2024-04-13T19:52:09.181358Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28985 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e0c7140ff54b388422da1bcbdb2eeb"}},"metadata":{}},{"name":"stdout","text":"DICE score: 0.9967882037162781, Jaccard score: 0.319852948188781743\r","output_type":"stream"}]},{"cell_type":"code","source":"train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard = train_net(ddp_model, EPOCHS, train_dataloader, validation_dataloader, optimizer, loss_function)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T19:47:59.497631Z","iopub.status.idle":"2024-04-13T19:47:59.498094Z","shell.execute_reply.started":"2024-04-13T19:47:59.497856Z","shell.execute_reply":"2024-04-13T19:47:59.497875Z"},"trusted":true},"execution_count":null,"outputs":[]}]}