{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.multiprocessing as mp","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:04.015253Z","iopub.execute_input":"2024-04-13T20:04:04.015584Z","iopub.status.idle":"2024-04-13T20:04:10.157784Z","shell.execute_reply.started":"2024-04-13T20:04:04.015534Z","shell.execute_reply":"2024-04-13T20:04:10.156468Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.159073Z","iopub.execute_input":"2024-04-13T20:04:10.159565Z","iopub.status.idle":"2024-04-13T20:04:10.167785Z","shell.execute_reply.started":"2024-04-13T20:04:10.159537Z","shell.execute_reply":"2024-04-13T20:04:10.166774Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.169105Z","iopub.execute_input":"2024-04-13T20:04:10.169472Z","iopub.status.idle":"2024-04-13T20:04:10.178754Z","shell.execute_reply.started":"2024-04-13T20:04:10.169443Z","shell.execute_reply":"2024-04-13T20:04:10.177736Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.181641Z","iopub.execute_input":"2024-04-13T20:04:10.182065Z","iopub.status.idle":"2024-04-13T20:04:10.220090Z","shell.execute_reply.started":"2024-04-13T20:04:10.182039Z","shell.execute_reply":"2024-04-13T20:04:10.219120Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalidation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.221281Z","iopub.execute_input":"2024-04-13T20:04:10.222013Z","iopub.status.idle":"2024-04-13T20:04:10.422598Z","shell.execute_reply.started":"2024-04-13T20:04:10.221982Z","shell.execute_reply":"2024-04-13T20:04:10.421760Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Sub Classes for U-Net and Attention U-Net","metadata":{}},{"cell_type":"code","source":"class conv_block(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.GroupNorm(32, ch_out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.GroupNorm(32, ch_out),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass resconv_block(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.GroupNorm(32, ch_out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.GroupNorm(32, ch_out),\n            nn.ReLU(inplace=True)\n        )\n        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n\n        return residual + x\n\n\nclass up_conv(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.GroupNorm(32, ch_out),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.423968Z","iopub.execute_input":"2024-04-13T20:04:10.424244Z","iopub.status.idle":"2024-04-13T20:04:10.438024Z","shell.execute_reply.started":"2024-04-13T20:04:10.424222Z","shell.execute_reply":"2024-04-13T20:04:10.436903Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# U-Net","metadata":{}},{"cell_type":"code","source":"class U_Net(nn.Module):\n    def __init__(self, img_ch=3, output_ch=1, first_layer_numKernel=64, name = \"U_Net\"):\n        super(U_Net, self).__init__()\n        self.name = name\n        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(ch_in=img_ch, ch_out=first_layer_numKernel)\n        self.Conv2 = conv_block(ch_in=first_layer_numKernel, ch_out=2 * first_layer_numKernel)\n        self.Conv3 = conv_block(ch_in=2 * first_layer_numKernel, ch_out=4 * first_layer_numKernel)\n        self.Conv4 = conv_block(ch_in=4 * first_layer_numKernel, ch_out=8 * first_layer_numKernel)\n        self.Conv5 = conv_block(ch_in=8 * first_layer_numKernel, ch_out=16 * first_layer_numKernel)\n\n        self.Up5 = up_conv(ch_in=16 * first_layer_numKernel, ch_out=8 * first_layer_numKernel)\n        self.Up_conv5 = conv_block(ch_in=16 * first_layer_numKernel, ch_out=8 * first_layer_numKernel)\n\n        self.Up4 = up_conv(ch_in=8 * first_layer_numKernel, ch_out=4 * first_layer_numKernel)\n        self.Up_conv4 = conv_block(ch_in=8 * first_layer_numKernel, ch_out=4 * first_layer_numKernel)\n\n        self.Up3 = up_conv(ch_in=4 * first_layer_numKernel, ch_out=2 * first_layer_numKernel)\n        self.Up_conv3 = conv_block(ch_in=4 * first_layer_numKernel, ch_out=2 * first_layer_numKernel)\n\n        self.Up2 = up_conv(ch_in=2 * first_layer_numKernel, ch_out=first_layer_numKernel)\n        self.Up_conv2 = conv_block(ch_in=2 * first_layer_numKernel, ch_out=first_layer_numKernel)\n\n        self.Conv_1x1 = nn.Sequential(\n            nn.Conv2d(first_layer_numKernel, output_ch, kernel_size=1, stride=1, padding=0), nn.Sigmoid()  # Use sigmoid activation for binary segmentation\n        )\n        # self.Conv_1x1 =  nn.Conv2d(first_layer_numKernel, output_ch, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self, x):\n\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x5 = self.Conv5(x5)\n\n        # decoding + concat path\n        d5 = self.Up5(x5)\n        d5 = torch.cat((x4, d5), dim=1)\n\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n        \n        #print(d1)\n\n        return d1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.439533Z","iopub.execute_input":"2024-04-13T20:04:10.439827Z","iopub.status.idle":"2024-04-13T20:04:10.459012Z","shell.execute_reply.started":"2024-04-13T20:04:10.439793Z","shell.execute_reply":"2024-04-13T20:04:10.457848Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class EarlyStopper:\n    def __init__(self, patience=1, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.max_validation_dice = float('-inf')\n\n    def early_stop(self, validation_dice):\n        if validation_dice > self.max_validation_dice:\n            self.max_validation_dice = validation_dice\n            self.counter = 0\n        elif validation_dice < (self.max_validation_dice + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:04:10.460776Z","iopub.execute_input":"2024-04-13T20:04:10.461107Z","iopub.status.idle":"2024-04-13T20:04:10.477197Z","shell.execute_reply.started":"2024-04-13T20:04:10.461081Z","shell.execute_reply":"2024-04-13T20:04:10.476119Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"LR = 5e-3\nEPOCHS = 10 # this maybe too big?\n\n\nunet = U_Net(img_ch=1, output_ch=1).to(device)\n\noptimizer = torch.optim.AdamW(unet.parameters(), lr=LR)\nloss_function = nn.BCELoss().to(device)\ndice_metric = Dice().to(device)\njaccard_index_metric = BinaryJaccardIndex().to(device)\n#early_stopper = EarlyStopper(patience=3, min_delta=0.02)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:07:24.732103Z","iopub.execute_input":"2024-04-13T20:07:24.732599Z","iopub.status.idle":"2024-04-13T20:07:25.151762Z","shell.execute_reply.started":"2024-04-13T20:07:24.732565Z","shell.execute_reply":"2024-04-13T20:07:25.150858Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(sum(p.numel() for p in unet.parameters() if p.requires_grad))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:07:25.153499Z","iopub.execute_input":"2024-04-13T20:07:25.153813Z","iopub.status.idle":"2024-04-13T20:07:25.160140Z","shell.execute_reply.started":"2024-04-13T20:07:25.153788Z","shell.execute_reply":"2024-04-13T20:07:25.158961Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"34525889\n","output_type":"stream"}]},{"cell_type":"code","source":"# The training function\ndef train_net(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function, dice_metric, jaccard_index_metric):\n\n    if not os.path.isdir('{0}'.format(net.name)):\n        os.mkdir('{0}'.format(net.name))\n    \n    n_train = len(train_dataloader)\n    n_valid = len(valid_dataloader)\n\n    train_loss, valid_loss = [], []\n    train_dice, valid_dice = [], []\n    train_jaccard, valid_jaccard = [], []\n\n    # Training\n    for epoch in range(epochs):\n        net.train()\n        train_batch_loss, train_batch_dice, train_batch_jaccard = [], [], []\n\n        for i, batch in enumerate(tqdm(train_dataloader)):\n            optimizer.zero_grad()\n            imgs = batch['image'].to(device).float()\n            true_masks = batch['mask'].to(device)\n\n            # Produce the estimated mask using current weights\n            y_pred = net(imgs)\n\n            loss = loss_function(y_pred, true_masks.float())\n            train_batch_loss.append(loss.cpu().item())\n            \n            y_pred = (y_pred >= 0.5).float()\n\n            batch_dice_score = dice_metric(y_pred, true_masks)\n            train_batch_dice.append(batch_dice_score)\n\n            batch_jaccard_score = jaccard_index_metric(y_pred, true_masks)\n            train_batch_jaccard.append(batch_jaccard_score)\n\n            loss.backward()\n            optimizer.step() \n            optimizer.zero_grad()\n\n            print(f'EPOCH {epoch + 1}/{epochs} - Training Batch {i+1}/{n_train} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}            ', end='\\r')\n        \n        \n\n        train_loss.append(np.mean(train_batch_loss))\n        train_dice.append(np.mean(train_batch_dice))\n        train_jaccard.append(np.mean(train_batch_jaccard))\n\n        net.eval()\n        valid_batch_loss, valid_batch_dice, valid_batch_jaccard = [], [], []\n\n        # Validation\n        with torch.no_grad():\n            for i, batch in enumerate(tqdm(valid_dataloader)):\n                imgs = batch['image'].to(device).float()\n                true_masks = batch['mask'] #.to(device).float()\n    \n                y_pred = net(imgs).cpu()\n\n                loss = loss_function(y_pred, true_masks.float())\n                valid_batch_loss.append(loss.item())\n                \n                y_pred = (y_pred >= 0.5).float()\n\n                batch_dice_score = dice_metric(y_pred, true_masks)\n                valid_batch_dice.append(batch_dice_score)\n\n                batch_jaccard_score = jaccard_index_metric(y_pred, true_masks)\n                valid_batch_jaccard.append(batch_jaccard_score)\n     \n                print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}            ', end='\\r')\n\n        valid_loss.append(np.mean(valid_batch_loss))\n        valid_dice.append(np.mean(valid_batch_dice))\n        valid_jaccard.append(np.mean(valid_batch_jaccard))\n        \n        #if early_stopper.early_stop(np.mean(valid_batch_dice)):             \n            #break \n\n        print(f'EPOCH {epoch + 1}/{epochs} - Training Loss: {np.mean(train_batch_loss)}, Training DICE score: {np.mean(train_batch_dice)}, Training Jaccard score: {np.mean(train_batch_jaccard)}, Validation Loss: {np.mean(valid_batch_loss)}, Validation DICE score: {np.mean(valid_batch_dice)}, Validation Jaccard score: {np.mean(valid_batch_jaccard)}')\n\n        torch.save(net.state_dict(), f'{net.name}/epoch_{epoch+1:3}.pth')\n\n    return train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:07:25.161676Z","iopub.execute_input":"2024-04-13T20:07:25.162190Z","iopub.status.idle":"2024-04-13T20:07:25.183536Z","shell.execute_reply.started":"2024-04-13T20:07:25.162165Z","shell.execute_reply":"2024-04-13T20:07:25.182235Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard = train_net(unet, EPOCHS, train_dataloader, validation_dataloader, optimizer, loss_function, dice_metric, jaccard_index_metric)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:07:25.185578Z","iopub.execute_input":"2024-04-13T20:07:25.185904Z","iopub.status.idle":"2024-04-13T20:11:46.139306Z","shell.execute_reply.started":"2024-04-13T20:07:25.185877Z","shell.execute_reply":"2024-04-13T20:11:46.137802Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8496 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b5bdbffb85e436389f9542875683ccf"}},"metadata":{}},{"name":"stdout","text":"EPOCH 1/10 - Training Batch 162/8496 - Loss: 0.051458247005939484, DICE score: 0.0, Jaccard score: 0.0            810940682888031            \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, train_dice, train_jaccard, valid_loss, valid_dice, valid_jaccard \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdice_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaccard_index_metric\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function, dice_metric, jaccard_index_metric)\u001b[0m\n\u001b[1;32m     16\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     17\u001b[0m train_batch_loss, train_batch_dice, train_batch_jaccard \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m     20\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     21\u001b[0m     true_masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mBraTSDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m----> 7\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m[index]\n\u001b[1;32m      8\u001b[0m     sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolder_path, file_name)))\n\u001b[1;32m      9\u001b[0m     img_as_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(sample[\u001b[38;5;241m0\u001b[39m,:,:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"#fixme: ","metadata":{}}]}