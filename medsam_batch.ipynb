{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550},{"sourceId":8141340,"sourceType":"datasetVersion","datasetId":4813464}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\nfrom functools import partial\nfrom pathlib import Path\nimport urllib.request\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\nfrom typing import Any, Optional, Tuple, Type, List, Dict\n\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:44:21.803553Z","iopub.execute_input":"2024-04-17T04:44:21.804184Z","iopub.status.idle":"2024-04-17T04:44:21.813427Z","shell.execute_reply.started":"2024-04-17T04:44:21.804149Z","shell.execute_reply":"2024-04-17T04:44:21.812315Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Model Loading","metadata":{}},{"cell_type":"code","source":"#!wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:43:14.865635Z","iopub.execute_input":"2024-04-17T04:43:14.865952Z","iopub.status.idle":"2024-04-17T04:43:14.874995Z","shell.execute_reply.started":"2024-04-17T04:43:14.865925Z","shell.execute_reply":"2024-04-17T04:43:14.873872Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class TwoWayTransformer(nn.Module):\n    def __init__(\n        self,\n        depth: int,\n        embedding_dim: int,\n        num_heads: int,\n        mlp_dim: int,\n        activation: Type[nn.Module] = nn.ReLU,\n        attention_downsample_rate: int = 2,\n    ) -> None:\n        \"\"\"\n        A transformer decoder that attends to an input image using\n        queries whose positional embedding is supplied.\n\n        Args:\n          depth (int): number of layers in the transformer\n          embedding_dim (int): the channel dimension for the input embeddings\n          num_heads (int): the number of heads for multihead attention. Must\n            divide embedding_dim\n          mlp_dim (int): the channel dimension internal to the MLP block\n          activation (nn.Module): the activation to use in the MLP block\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.mlp_dim = mlp_dim\n        self.layers = nn.ModuleList()\n\n        for i in range(depth):\n            self.layers.append(\n                TwoWayAttentionBlock(\n                    embedding_dim=embedding_dim,\n                    num_heads=num_heads,\n                    mlp_dim=mlp_dim,\n                    activation=activation,\n                    attention_downsample_rate=attention_downsample_rate,\n                    skip_first_layer_pe=(i == 0),\n                )\n            )\n\n        self.final_attn_token_to_image = Attention_Down(\n            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n        )\n        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n\n    def forward(\n        self,\n        image_embedding: torch.Tensor,\n        image_pe: torch.Tensor,\n        point_embedding: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n          image_embedding (torch.Tensor): image to attend to. Should be shape\n            B x embedding_dim x h x w for any h and w.\n          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n            have the same shape as image_embedding.\n          point_embedding (torch.Tensor): the embedding to add to the query points.\n            Must have shape B x N_points x embedding_dim for any N_points.\n\n        Returns:\n          torch.Tensor: the processed point_embedding\n          torch.Tensor: the processed image_embedding\n        \"\"\"\n        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n        bs, c, h, w = image_embedding.shape\n        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n\n        # Prepare queries\n        queries = point_embedding\n        keys = image_embedding\n\n        # Apply transformer blocks and final layernorm\n        for layer in self.layers:\n            queries, keys = layer(\n                queries=queries,\n                keys=keys,\n                query_pe=point_embedding,\n                key_pe=image_pe,\n            )\n\n        # Apply the final attention layer from the points to the image\n        q = queries + point_embedding\n        k = keys + image_pe\n        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n        queries = queries + attn_out\n        queries = self.norm_final_attn(queries)\n\n        return queries, keys\n\n\nclass TwoWayAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        mlp_dim: int = 2048,\n        activation: Type[nn.Module] = nn.ReLU,\n        attention_downsample_rate: int = 2,\n        skip_first_layer_pe: bool = False,\n    ) -> None:\n        \"\"\"\n        A transformer block with four layers: (1) self-attention of sparse\n        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n        inputs.\n\n        Arguments:\n          embedding_dim (int): the channel dimension of the embeddings\n          num_heads (int): the number of heads in the attention layers\n          mlp_dim (int): the hidden dimension of the mlp block\n          activation (nn.Module): the activation of the mlp block\n          skip_first_layer_pe (bool): skip the PE on the first layer\n        \"\"\"\n        super().__init__()\n        self.self_attn = Attention_Down(embedding_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embedding_dim)\n\n        self.cross_attn_token_to_image = Attention_Down(\n            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n        )\n        self.norm2 = nn.LayerNorm(embedding_dim)\n\n        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n        self.norm3 = nn.LayerNorm(embedding_dim)\n\n        self.norm4 = nn.LayerNorm(embedding_dim)\n        self.cross_attn_image_to_token = Attention_Down(\n            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n        )\n\n        self.skip_first_layer_pe = skip_first_layer_pe\n\n    def forward(\n        self, queries: torch.Tensor, keys: torch.Tensor, query_pe: torch.Tensor, key_pe: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self attention block\n        if self.skip_first_layer_pe:\n            queries = self.self_attn(q=queries, k=queries, v=queries)\n        else:\n            q = queries + query_pe\n            attn_out = self.self_attn(q=q, k=q, v=queries)\n            queries = queries + attn_out\n        queries = self.norm1(queries)\n\n        # Cross attention block, tokens attending to image embedding\n        q = queries + query_pe\n        k = keys + key_pe\n        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n        queries = queries + attn_out\n        queries = self.norm2(queries)\n\n        # MLP block\n        mlp_out = self.mlp(queries)\n        queries = queries + mlp_out\n        queries = self.norm3(queries)\n\n        # Cross attention block, image embedding attending to tokens\n        q = queries + query_pe\n        k = keys + key_pe\n        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n        keys = keys + attn_out\n        keys = self.norm4(keys)\n\n        return queries, keys\n\n\nclass Attention_Down(nn.Module):\n    \"\"\"\n    An attention layer that allows for downscaling the size of the embedding\n    after projection to queries, keys, and values.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        downsample_rate: int = 1,\n    ) -> None:\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.internal_dim = embedding_dim // downsample_rate\n        self.num_heads = num_heads\n        assert (\n            self.internal_dim % num_heads == 0\n        ), \"num_heads must divide embedding_dim.\"\n\n        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n\n    def _separate_heads(self, x: torch.Tensor, num_heads: int) -> torch.Tensor:\n        b, n, c = x.shape\n        x = x.reshape(b, n, num_heads, c // num_heads)\n        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n\n    def _recombine_heads(self, x: torch.Tensor) -> torch.Tensor:\n        b, n_heads, n_tokens, c_per_head = x.shape\n        x = x.transpose(1, 2)\n        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n        # Input projections\n        q = self.q_proj(q)\n        k = self.k_proj(k)\n        v = self.v_proj(v)\n\n        # Separate into heads\n        q = self._separate_heads(q, self.num_heads)\n        k = self._separate_heads(k, self.num_heads)\n        v = self._separate_heads(v, self.num_heads)\n\n        # Attention\n        _, _, _, c_per_head = q.shape\n        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n        attn = attn / math.sqrt(c_per_head)\n        attn = torch.softmax(attn, dim=-1)\n\n        # Get output\n        out = attn @ v\n        out = self._recombine_heads(out)\n        out = self.out_proj(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:43:14.877290Z","iopub.execute_input":"2024-04-17T04:43:14.877672Z","iopub.status.idle":"2024-04-17T04:43:14.919789Z","shell.execute_reply.started":"2024-04-17T04:43:14.877638Z","shell.execute_reply":"2024-04-17T04:43:14.918766Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Common","metadata":{}},{"cell_type":"code","source":"class MLPBlock(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        mlp_dim: int,\n        act: Type[nn.Module] = nn.GELU,\n    ) -> None:\n        super().__init__()\n        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n        self.act = act()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.lin2(self.act(self.lin1(x)))\n\n\n# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\nclass LayerNorm2d(nn.Module):\n    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(num_channels))\n        self.bias = nn.Parameter(torch.zeros(num_channels))\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        u = x.mean(1, keepdim=True)\n        s = (x - u).pow(2).mean(1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.eps)\n        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:43:14.921192Z","iopub.execute_input":"2024-04-17T04:43:14.922358Z","iopub.status.idle":"2024-04-17T04:43:14.936231Z","shell.execute_reply.started":"2024-04-17T04:43:14.922316Z","shell.execute_reply":"2024-04-17T04:43:14.935216Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Image Encoder","metadata":{}},{"cell_type":"code","source":"class ImageEncoderViT(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 1024,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        out_chans: int = 256,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_abs_pos: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        global_attn_indexes: Tuple[int, ...] = (),\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\n        super().__init__()\n        self.img_size = img_size\n\n        self.patch_embed = PatchEmbed(\n            kernel_size=(patch_size, patch_size),\n            stride=(patch_size, patch_size),\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n\n        self.pos_embed: Optional[nn.Parameter] = None\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(\n                torch.zeros(\n                    1, img_size // patch_size, img_size // patch_size, embed_dim\n                )\n            )\n\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=(img_size // patch_size, img_size // patch_size),\n            )\n            self.blocks.append(block)\n\n        self.neck = nn.Sequential(\n            nn.Conv2d(\n                embed_dim,\n                out_chans,\n                kernel_size=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n            nn.Conv2d(\n                out_chans,\n                out_chans,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.neck(x.permute(0, 3, 1, 2))\n\n        return x\n\n\nclass Block(nn.Module):\n    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks. If it equals 0, then\n                use global attention.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            use_rel_pos=use_rel_pos,\n            rel_pos_zero_init=rel_pos_zero_init,\n            input_size=input_size if window_size == 0 else (window_size, window_size),\n        )\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = MLPBlock(\n            embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer\n        )\n\n        self.window_size = window_size\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n        x = self.norm1(x)\n        # Window partition\n        if self.window_size > 0:\n            H, W = x.shape[1], x.shape[2]\n            x, pad_hw = window_partition(x, self.window_size)\n\n        x = self.attn(x)\n        # Reverse window partition\n        if self.window_size > 0:\n            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n\n        x = shortcut + x\n        x = x + self.mlp(self.norm2(x))\n\n        return x\n\n\nclass Attention(nn.Module):\n    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads.\n            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n\n        self.use_rel_pos = use_rel_pos\n        if self.use_rel_pos:\n            assert (\n                input_size is not None\n            ), \"Input size must be provided if using relative positional encoding.\"\n            # initialize relative positional embeddings\n            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, H, W, _ = x.shape\n        # qkv with shape (3, B, nHead, H * W, C)\n        qkv = (\n            self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        )\n        # q, k, v with shape (B * nHead, H * W, C)\n        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n\n        if self.use_rel_pos:\n            attn = add_decomposed_rel_pos(\n                attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W)\n            )\n\n        attn = attn.softmax(dim=-1)\n        x = (\n            (attn @ v)\n            .view(B, self.num_heads, H, W, -1)\n            .permute(0, 2, 3, 1, 4)\n            .reshape(B, H, W, -1)\n        )\n        x = self.proj(x)\n\n        return x\n\n\ndef window_partition(\n    x: torch.Tensor, window_size: int\n) -> Tuple[torch.Tensor, Tuple[int, int]]:\n    \"\"\"\n    Partition into non-overlapping windows with padding if needed.\n    Args:\n        x (tensor): input tokens with [B, H, W, C].\n        window_size (int): window size.\n\n    Returns:\n        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n        (Hp, Wp): padded height and width before partition\n    \"\"\"\n    B, H, W, C = x.shape\n\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n    Hp, Wp = H + pad_h, W + pad_w\n\n    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    )\n    return windows, (Hp, Wp)\n\n\ndef window_unpartition(\n    windows: torch.Tensor,\n    window_size: int,\n    pad_hw: Tuple[int, int],\n    hw: Tuple[int, int],\n) -> torch.Tensor:\n    \"\"\"\n    Window unpartition into original sequences and removing padding.\n    Args:\n        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n        window_size (int): window size.\n        pad_hw (Tuple): padded height and width (Hp, Wp).\n        hw (Tuple): original height and width (H, W) before padding.\n\n    Returns:\n        x: unpartitioned sequences with [B, H, W, C].\n    \"\"\"\n    Hp, Wp = pad_hw\n    H, W = hw\n    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n    x = windows.view(\n        B, Hp // window_size, Wp // window_size, window_size, window_size, -1\n    )\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n\n    if Hp > H or Wp > W:\n        x = x[:, :H, :W, :].contiguous()\n    return x\n\n\ndef get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get relative positional embeddings according to the relative positions of\n        query and key sizes.\n    Args:\n        q_size (int): size of query q.\n        k_size (int): size of key k.\n        rel_pos (Tensor): relative position embeddings (L, C).\n\n    Returns:\n        Extracted positional embeddings according to relative positions.\n    \"\"\"\n    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n    # Interpolate rel pos if needed.\n    if rel_pos.shape[0] != max_rel_dist:\n        # Interpolate rel pos.\n        rel_pos_resized = F.interpolate(\n            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n            size=max_rel_dist,\n            mode=\"linear\",\n        )\n        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n    else:\n        rel_pos_resized = rel_pos\n\n    # Scale the coords with short length if shapes for q and k are different.\n    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n\n    return rel_pos_resized[relative_coords.long()]\n\n\ndef add_decomposed_rel_pos(\n    attn: torch.Tensor,\n    q: torch.Tensor,\n    rel_pos_h: torch.Tensor,\n    rel_pos_w: torch.Tensor,\n    q_size: Tuple[int, int],\n    k_size: Tuple[int, int],\n) -> torch.Tensor:\n    \"\"\"\n    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n    Args:\n        attn (Tensor): attention map.\n        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n\n    Returns:\n        attn (Tensor): attention map with added relative positional embeddings.\n    \"\"\"\n    q_h, q_w = q_size\n    k_h, k_w = k_size\n    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n\n    B, _, dim = q.shape\n    r_q = q.reshape(B, q_h, q_w, dim)\n    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n\n    attn = (\n        attn.view(B, q_h, q_w, k_h, k_w)\n        + rel_h[:, :, :, :, None]\n        + rel_w[:, :, :, None, :]\n    ).view(B, q_h * q_w, k_h * k_w)\n\n    return attn\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_size: Tuple[int, int] = (16, 16),\n        stride: Tuple[int, int] = (16, 16),\n        padding: Tuple[int, int] = (0, 0),\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -> None:\n        \"\"\"\n        Args:\n            kernel_size (Tuple): kernel size of the projection layer.\n            stride (Tuple): stride of the projection layer.\n            padding (Tuple): padding size of the projection layer.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n        \"\"\"\n        super().__init__()\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.proj(x)\n        # B C H W -> B H W C\n        x = x.permute(0, 2, 3, 1)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Decoder","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nclass MaskDecoder(nn.Module):\n    def __init__(\n        self,\n        *,\n        transformer_dim: int,\n        transformer: nn.Module,\n        num_multimask_outputs: int = 3,\n        activation: Type[nn.Module] = nn.GELU,\n        iou_head_depth: int = 3,\n        iou_head_hidden_dim: int = 256,\n    ) -> None:\n        \"\"\"\n        Predicts masks given an image and prompt embeddings, using a\n        transformer architecture.\n\n        Arguments:\n          transformer_dim (int): the channel dimension of the transformer\n          transformer (nn.Module): the transformer used to predict masks\n          num_multimask_outputs (int): the number of masks to predict\n            when disambiguating masks\n          activation (nn.Module): the type of activation to use when\n            upscaling masks\n          iou_head_depth (int): the depth of the MLP used to predict\n            mask quality\n          iou_head_hidden_dim (int): the hidden dimension of the MLP\n            used to predict mask quality\n        \"\"\"\n        super().__init__()\n        self.transformer_dim = transformer_dim\n        self.transformer = transformer\n\n        self.num_multimask_outputs = num_multimask_outputs\n\n        self.iou_token = nn.Embedding(1, transformer_dim)\n        self.num_mask_tokens = num_multimask_outputs + 1\n        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n\n        self.output_upscaling = nn.Sequential(\n            nn.ConvTranspose2d(\n                transformer_dim, transformer_dim // 4, kernel_size=2, stride=2\n            ),\n            LayerNorm2d(transformer_dim // 4),\n            activation(),\n            nn.ConvTranspose2d(\n                transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2\n            ),\n            activation(),\n        )\n        self.output_hypernetworks_mlps = nn.ModuleList(\n            [\n                MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n                for i in range(self.num_mask_tokens)\n            ]\n        )\n\n        self.iou_prediction_head = MLP(\n            transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth\n        )\n\n    def forward(\n        self,\n        image_embeddings: torch.Tensor,\n        image_pe: torch.Tensor,\n        sparse_prompt_embeddings: torch.Tensor,\n        dense_prompt_embeddings: torch.Tensor,\n        multimask_output: bool,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Predict masks given image and prompt embeddings.\n\n        Arguments:\n          image_embeddings (torch.Tensor): the embeddings from the image encoder\n          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n          multimask_output (bool): Whether to return multiple masks or a single\n            mask.\n\n        Returns:\n          torch.Tensor: batched predicted masks\n          torch.Tensor: batched predictions of mask quality\n        \"\"\"\n        masks, iou_pred = self.predict_masks(\n            image_embeddings=image_embeddings,\n            image_pe=image_pe,\n            sparse_prompt_embeddings=sparse_prompt_embeddings,\n            dense_prompt_embeddings=dense_prompt_embeddings,\n        )\n\n        # Select the correct mask or masks for output\n        if multimask_output:\n            mask_slice = slice(1, None)\n        else:\n            mask_slice = slice(0, 1)\n        masks = masks[:, mask_slice, :, :]\n        iou_pred = iou_pred[:, mask_slice]\n\n        # Prepare output\n        return masks, iou_pred\n\n    def predict_masks(\n        self,\n        image_embeddings: torch.Tensor,\n        image_pe: torch.Tensor,\n        sparse_prompt_embeddings: torch.Tensor,\n        dense_prompt_embeddings: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n        # Concatenate output tokens\n        output_tokens = torch.cat(\n            [self.iou_token.weight, self.mask_tokens.weight], dim=0\n        )\n        output_tokens = output_tokens.unsqueeze(0).expand(\n            sparse_prompt_embeddings.size(0), -1, -1\n        )\n        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n\n        # Expand per-image data in batch direction to be per-mask\n#         if image_embeddings.shape[0] != tokens.shape[0]:\n#             src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n#         else:\n#             src = image_embeddings\n        src = image_embeddings + dense_prompt_embeddings\n        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n        b, c, h, w = src.shape\n\n        # Run the transformer\n        hs, src = self.transformer(src, pos_src, tokens)\n        iou_token_out = hs[:, 0, :]\n        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n\n        # Upscale mask embeddings and predict masks using the mask tokens\n        src = src.transpose(1, 2).view(b, c, h, w)\n        upscaled_embedding = self.output_upscaling(src)\n        hyper_in_list: List[torch.Tensor] = []\n        for i in range(self.num_mask_tokens):\n            hyper_in_list.append(\n                self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])\n            )\n        hyper_in = torch.stack(hyper_in_list, dim=1)\n        b, c, h, w = upscaled_embedding.shape\n        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n\n        # Generate mask quality predictions\n        iou_pred = self.iou_prediction_head(iou_token_out)\n\n        return masks, iou_pred\n\n\n# Lightly adapted from\n# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        sigmoid_output: bool = False,\n    ) -> None:\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(\n            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n        )\n        self.sigmoid_output = sigmoid_output\n\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n        if self.sigmoid_output:\n            x = F.sigmoid(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mask Decoder","metadata":{}},{"cell_type":"code","source":"class PromptEncoder(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        image_embedding_size: Tuple[int, int],\n        input_image_size: Tuple[int, int],\n        mask_in_chans: int,\n        activation: Type[nn.Module] = nn.GELU,\n    ) -> None:\n        \"\"\"\n        Encodes prompts for input to SAM's mask decoder.\n\n        Arguments:\n          embed_dim (int): The prompts' embedding dimension\n          image_embedding_size (tuple(int, int)): The spatial size of the\n            image embedding, as (H, W).\n          input_image_size (int): The padded size of the image as input\n            to the image encoder, as (H, W).\n          mask_in_chans (int): The number of hidden channels used for\n            encoding input masks.\n          activation (nn.Module): The activation to use when encoding\n            input masks.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.input_image_size = input_image_size\n        self.image_embedding_size = image_embedding_size\n        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n\n        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n        point_embeddings = [\n            nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)\n        ]\n        self.point_embeddings = nn.ModuleList(point_embeddings)\n        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n\n        self.mask_input_size = (\n            4 * image_embedding_size[0],\n            4 * image_embedding_size[1],\n        )\n        self.mask_downscaling = nn.Sequential(\n            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n            LayerNorm2d(mask_in_chans // 4),\n            activation(),\n            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n            LayerNorm2d(mask_in_chans),\n            activation(),\n            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n        )\n        self.no_mask_embed = nn.Embedding(1, embed_dim)\n\n    def get_dense_pe(self) -> torch.Tensor:\n        \"\"\"\n        Returns the positional encoding used to encode point prompts,\n        applied to a dense set of points the shape of the image encoding.\n\n        Returns:\n          torch.Tensor: Positional encoding with shape\n            1x(embed_dim)x(embedding_h)x(embedding_w)\n        \"\"\"\n        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n\n    def _embed_points(\n        self,\n        points: torch.Tensor,\n        labels: torch.Tensor,\n        pad: bool,\n    ) -> torch.Tensor:\n        \"\"\"Embeds point prompts.\"\"\"\n        points = points + 0.5  # Shift to center of pixel\n        if pad:\n            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n            points = torch.cat([points, padding_point], dim=1)\n            labels = torch.cat([labels, padding_label], dim=1)\n        point_embedding = self.pe_layer.forward_with_coords(\n            points, self.input_image_size\n        )\n        point_embedding[labels == -1] = 0.0\n        point_embedding[labels == -1] += self.not_a_point_embed.weight\n        point_embedding[labels == 0] += self.point_embeddings[0].weight\n        point_embedding[labels == 1] += self.point_embeddings[1].weight\n        return point_embedding\n\n    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n        \"\"\"Embeds box prompts.\"\"\"\n        boxes = boxes + 0.5  # Shift to center of pixel\n        coords = boxes.reshape(-1, 2, 2)\n        corner_embedding = self.pe_layer.forward_with_coords(\n            coords, self.input_image_size\n        )\n        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n        return corner_embedding\n\n    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n        \"\"\"Embeds mask inputs.\"\"\"\n        mask_embedding = self.mask_downscaling(masks)\n        return mask_embedding\n\n    def _get_batch_size(\n        self,\n        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n        boxes: Optional[torch.Tensor],\n        masks: Optional[torch.Tensor],\n    ) -> int:\n        \"\"\"\n        Gets the batch size of the output given the batch size of the input prompts.\n        \"\"\"\n        if points is not None:\n            return points[0].shape[0]\n        elif boxes is not None:\n            return boxes.shape[0]\n        elif masks is not None:\n            return masks.shape[0]\n        else:\n            return 1\n\n    def _get_device(self) -> torch.device:\n        return self.point_embeddings[0].weight.device\n\n    def forward(\n        self,\n        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n        boxes: Optional[torch.Tensor],\n        masks: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Embeds different types of prompts, returning both sparse and dense\n        embeddings.\n\n        Arguments:\n          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n            and labels to embed.\n          boxes (torch.Tensor or none): boxes to embed\n          masks (torch.Tensor or none): masks to embed\n\n        Returns:\n          torch.Tensor: sparse embeddings for the points and boxes, with shape\n            BxNx(embed_dim), where N is determined by the number of input points\n            and boxes.\n          torch.Tensor: dense embeddings for the masks, in the shape\n            Bx(embed_dim)x(embed_H)x(embed_W)\n        \"\"\"\n        bs = self._get_batch_size(points, boxes, masks)\n        sparse_embeddings = torch.empty(\n            (bs, 0, self.embed_dim), device=self._get_device()\n        )\n        if points is not None:\n            coords, labels = points\n            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n        if boxes is not None:\n            box_embeddings = self._embed_boxes(boxes)\n            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n\n        if masks is not None:\n            dense_embeddings = self._embed_masks(masks)\n        else:\n            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n            )\n\n        return sparse_embeddings, dense_embeddings\n\n\nclass PositionEmbeddingRandom(nn.Module):\n    \"\"\"\n    Positional encoding using random spatial frequencies.\n    \"\"\"\n\n    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n        super().__init__()\n        if scale is None or scale <= 0.0:\n            scale = 1.0\n        self.register_buffer(\n            \"positional_encoding_gaussian_matrix\",\n            scale * torch.randn((2, num_pos_feats)),\n        )\n\n    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n        coords = 2 * coords - 1\n        coords = coords @ self.positional_encoding_gaussian_matrix\n        coords = 2 * np.pi * coords\n        # outputs d_1 x ... x d_n x C shape\n        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n\n    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n        h, w = size\n        device: Any = self.positional_encoding_gaussian_matrix.device\n        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n        y_embed = grid.cumsum(dim=0) - 0.5\n        x_embed = grid.cumsum(dim=1) - 0.5\n        y_embed = y_embed / h\n        x_embed = x_embed / w\n\n        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n        return pe.permute(2, 0, 1)  # C x H x W\n\n    def forward_with_coords(\n        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n    ) -> torch.Tensor:\n        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n        coords = coords_input.clone()\n        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n        return self._pe_encoding(coords.to(torch.float))  # B x N x C","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"code","source":"class Sam(nn.Module):\n    mask_threshold: float = 0.0\n    image_format: str = \"RGB\"\n\n    def __init__(\n        self,\n        image_encoder: ImageEncoderViT,\n        prompt_encoder: PromptEncoder,\n        mask_decoder: MaskDecoder,\n        pixel_mean: List[float] = [123.675, 116.28, 103.53],\n        pixel_std: List[float] = [58.395, 57.12, 57.375],\n    ) -> None:\n        \"\"\"\n        SAM predicts object masks from an image and input prompts.\n\n        Arguments:\n          image_encoder (ImageEncoderViT): The backbone used to encode the\n            image into image embeddings that allow for efficient mask prediction.\n          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n          mask_decoder (MaskDecoder): Predicts masks from the image embeddings\n            and encoded prompts.\n          pixel_mean (list(float)): Mean values for normalizing pixels in the input image.\n          pixel_std (list(float)): Std values for normalizing pixels in the input image.\n        \"\"\"\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.prompt_encoder = prompt_encoder\n        self.mask_decoder = mask_decoder\n        self.register_buffer(\n            \"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False\n        )\n        self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n\n    @property\n    def device(self) -> Any:\n        return self.pixel_mean.device\n\n    @torch.no_grad()\n    def forward(\n        self,\n        batched_input: List[Dict[str, Any]],\n        multimask_output: bool,\n    ) -> List[Dict[str, torch.Tensor]]:\n        \"\"\"\n        Predicts masks end-to-end from provided images and prompts.\n        If prompts are not known in advance, using SamPredictor is\n        recommended over calling the model directly.\n\n        Arguments:\n          batched_input (list(dict)): A list over input images, each a\n            dictionary with the following keys. A prompt key can be\n            excluded if it is not present.\n              'image': The image as a torch tensor in 3xHxW format,\n                already transformed for input to the model.\n              'original_size': (tuple(int, int)) The original size of\n                the image before transformation, as (H, W).\n              'point_coords': (torch.Tensor) Batched point prompts for\n                this image, with shape BxNx2. Already transformed to the\n                input frame of the model.\n              'point_labels': (torch.Tensor) Batched labels for point prompts,\n                with shape BxN.\n              'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n                Already transformed to the input frame of the model.\n              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n                in the form Bx1xHxW.\n          multimask_output (bool): Whether the model should predict multiple\n            disambiguating masks, or return a single mask.\n\n        Returns:\n          (list(dict)): A list over input images, where each element is\n            as dictionary with the following keys.\n              'masks': (torch.Tensor) Batched binary mask predictions,\n                with shape BxCxHxW, where B is the number of input prompts,\n                C is determined by multimask_output, and (H, W) is the\n                original size of the image.\n              'iou_predictions': (torch.Tensor) The model's predictions\n                of mask quality, in shape BxC.\n              'low_res_logits': (torch.Tensor) Low resolution logits with\n                shape BxCxHxW, where H=W=256. Can be passed as mask input\n                to subsequent iterations of prediction.\n        \"\"\"\n        input_images = torch.stack(\n            [self.preprocess(x[\"image\"]) for x in batched_input], dim=0\n        )\n        image_embeddings = self.image_encoder(input_images)\n\n        outputs = []\n        for image_record, curr_embedding in zip(batched_input, image_embeddings):\n            if \"point_coords\" in image_record:\n                points = (image_record[\"point_coords\"], image_record[\"point_labels\"])\n            else:\n                points = None\n            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n                points=points,\n                boxes=image_record.get(\"boxes\", None),\n                masks=image_record.get(\"mask_inputs\", None),\n            )\n            low_res_masks, iou_predictions = self.mask_decoder(\n                image_embeddings=curr_embedding.unsqueeze(0),\n                image_pe=self.prompt_encoder.get_dense_pe(),\n                sparse_prompt_embeddings=sparse_embeddings,\n                dense_prompt_embeddings=dense_embeddings,\n                multimask_output=multimask_output,\n            )\n            masks = self.postprocess_masks(\n                low_res_masks,\n                input_size=image_record[\"image\"].shape[-2:],\n                original_size=image_record[\"original_size\"],\n            )\n            masks = masks > self.mask_threshold\n            outputs.append(\n                {\n                    \"masks\": masks,\n                    \"iou_predictions\": iou_predictions,\n                    \"low_res_logits\": low_res_masks,\n                }\n            )\n        return outputs\n\n    def postprocess_masks(\n        self,\n        masks: torch.Tensor,\n        input_size: Tuple[int, ...],\n        original_size: Tuple[int, ...],\n    ) -> torch.Tensor:\n        \"\"\"\n        Remove padding and upscale masks to the original image size.\n\n        Arguments:\n          masks (torch.Tensor): Batched masks from the mask_decoder,\n            in BxCxHxW format.\n          input_size (tuple(int, int)): The size of the image input to the\n            model, in (H, W) format. Used to remove padding.\n          original_size (tuple(int, int)): The original size of the image\n            before resizing for input to the model, in (H, W) format.\n\n        Returns:\n          (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n            is given by original_size.\n        \"\"\"\n        masks = F.interpolate(\n            masks,\n            (self.image_encoder.img_size, self.image_encoder.img_size),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        masks = masks[..., : input_size[0], : input_size[1]]\n        masks = F.interpolate(\n            masks, original_size, mode=\"bilinear\", align_corners=False\n        )\n        return masks\n\n    def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n        # Normalize colors\n        x = (x - self.pixel_mean) / self.pixel_std\n\n        # Pad\n        h, w = x.shape[-2:]\n        padh = self.image_encoder.img_size - h\n        padw = self.image_encoder.img_size - w\n        x = F.pad(x, (0, padw, 0, padh))\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class NpyDataset(Dataset):\n    def __init__(self, data_root, bbox_shift=20):\n        self.data_root = data_root\n        self.gt_path = self.data_root\n        self.img_path = self.data_root\n        self.gt_path_files = sorted(\n            glob.glob(join(self.data_root, \"**/*.npy\"), recursive=True)\n        )\n        self.gt_path_files = [\n            file\n            for file in self.gt_path_files\n            if os.path.isfile(join(self.img_path, os.path.basename(file)))\n        ]\n        self.bbox_shift = bbox_shift\n        print(f\"number of images: {len(self.gt_path_files)}\")\n\n    def __len__(self):\n        return len(self.gt_path_files)\n\n    def __getitem__(self, index):\n        # load npy image (240, 240)\n        img_name = os.path.basename(self.gt_path_files[index])    \n        sample = np.load(\n            join(self.img_path, img_name), \"r\", allow_pickle=True\n        )  # (240, 240)\n         \n        img_1024 = sample[0,:,:]\n        img_1024 = transform.resize(img_1024, (1024,1024), order=3, preserve_range=True, \n                           anti_aliasing=True).astype(np.uint8)\n        img_1024 = np.tile(img_1024, (3,1,1))\n\n        assert (\n            np.max(img_1024) <= 1.0 and np.min(img_1024) >= 0.0\n        ), \"image should be normalized to [0, 1]\"\n        \n        gt = sample[1,:,:]\n#         assert img_name == os.path.basename(self.gt_path_files[index]), (\n#             \"img gt name error\" + self.gt_path_files[index] + self.npy_files[index]\n#         )\n        label_ids = np.unique(gt)[1:]\n        gt2D = np.uint8(\n            gt == random.choice(label_ids.tolist())\n        )  # only one label, (256, 256)\n        assert np.max(gt2D) == 1 and np.min(gt2D) == 0.0, \"ground truth should be 0, 1\"\n        y_indices, x_indices = np.where(gt2D > 0)\n        x_min, x_max = np.min(x_indices), np.max(x_indices)\n        y_min, y_max = np.min(y_indices), np.max(y_indices)\n        # add perturbation to bounding box coordinates\n        H, W = gt2D.shape\n        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n        bboxes = np.array([x_min, y_min, x_max, y_max])\n        return (\n            torch.tensor(img_1024).float(),\n            torch.tensor(gt2D[None, :, :]).long(),\n            torch.tensor(bboxes).float(),\n            img_name,\n        )","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:30:59.896535Z","iopub.execute_input":"2024-04-17T04:30:59.896850Z","iopub.status.idle":"2024-04-17T04:30:59.919775Z","shell.execute_reply.started":"2024-04-17T04:30:59.896824Z","shell.execute_reply":"2024-04-17T04:30:59.918753Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/brats-dataset-medsam/full_raw_medsam/train/medsam'\ntrain_dataset = NpyDataset(data_root = data_root_folder)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:30:59.920920Z","iopub.execute_input":"2024-04-17T04:30:59.921258Z","iopub.status.idle":"2024-04-17T04:31:14.513753Z","shell.execute_reply.started":"2024-04-17T04:30:59.921232Z","shell.execute_reply":"2024-04-17T04:31:14.512708Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"number of images: 57351\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:14.515796Z","iopub.execute_input":"2024-04-17T04:31:14.516122Z","iopub.status.idle":"2024-04-17T04:31:14.521824Z","shell.execute_reply.started":"2024-04-17T04:31:14.516093Z","shell.execute_reply":"2024-04-17T04:31:14.520783Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"next(iter(train_dataloader))[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:14.523039Z","iopub.execute_input":"2024-04-17T04:31:14.523456Z","iopub.status.idle":"2024-04-17T04:31:15.066382Z","shell.execute_reply.started":"2024-04-17T04:31:14.523371Z","shell.execute_reply":"2024-04-17T04:31:15.065270Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 3, 1024, 1024])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:29.427143Z","iopub.execute_input":"2024-04-17T04:31:29.427545Z","iopub.status.idle":"2024-04-17T04:31:29.463001Z","shell.execute_reply.started":"2024-04-17T04:31:29.427512Z","shell.execute_reply":"2024-04-17T04:31:29.461820Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:30.902996Z","iopub.execute_input":"2024-04-17T04:31:30.903385Z","iopub.status.idle":"2024-04-17T04:31:30.908939Z","shell.execute_reply.started":"2024-04-17T04:31:30.903352Z","shell.execute_reply":"2024-04-17T04:31:30.907881Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:31.780005Z","iopub.execute_input":"2024-04-17T04:31:31.780383Z","iopub.status.idle":"2024-04-17T04:31:31.789857Z","shell.execute_reply.started":"2024-04-17T04:31:31.780351Z","shell.execute_reply":"2024-04-17T04:31:31.788784Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:33.366347Z","iopub.execute_input":"2024-04-17T04:31:33.366758Z","iopub.status.idle":"2024-04-17T04:31:33.374666Z","shell.execute_reply.started":"2024-04-17T04:31:33.366726Z","shell.execute_reply":"2024-04-17T04:31:33.373568Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    W = 240\n    H = 240\n    margin = 5\n    box_1024_batch = []\n    useless_image_indices = []\n    \n    for ind, mask in enumerate(batch_mask):\n        mask = mask[0]\n        nonzero_indices = torch.nonzero(mask).T\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n#             #box_1024 = torch.asarray([[-1, -1, -1, -1]])\n        else:\n            min_y = torch.min(nonzero_indices[0, :]).values\n            max_y = torch.max(nonzero_indices[0, :]).values\n            min_x = torch.min(nonzero_indices[1, :]).values\n            max_x = torch.max(nonzero_indices[1, :]).values\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n            box_1024_batch.append(box_1024)\n    \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:34.261042Z","iopub.execute_input":"2024-04-17T04:31:34.261626Z","iopub.status.idle":"2024-04-17T04:31:34.272246Z","shell.execute_reply.started":"2024-04-17T04:31:34.261581Z","shell.execute_reply":"2024-04-17T04:31:34.270935Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:36.250048Z","iopub.execute_input":"2024-04-17T04:31:36.251098Z","iopub.status.idle":"2024-04-17T04:31:36.259543Z","shell.execute_reply.started":"2024-04-17T04:31:36.251061Z","shell.execute_reply":"2024-04-17T04:31:36.258388Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def _build_sam(\n    encoder_embed_dim,\n    encoder_depth,\n    encoder_num_heads,\n    encoder_global_attn_indexes,\n    checkpoint=None,\n):\n    prompt_embed_dim = 256\n    image_size = 1024\n    vit_patch_size = 16\n    image_embedding_size = image_size // vit_patch_size\n    sam = Sam(\n        image_encoder=ImageEncoderViT(\n            depth=encoder_depth,\n            embed_dim=encoder_embed_dim,\n            img_size=image_size,\n            mlp_ratio=4,\n            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n            num_heads=encoder_num_heads,\n            patch_size=vit_patch_size,\n            qkv_bias=True,\n            use_rel_pos=True,\n            global_attn_indexes=encoder_global_attn_indexes,\n            window_size=14,\n            out_chans=prompt_embed_dim,\n        ),\n        prompt_encoder=PromptEncoder(\n            embed_dim=prompt_embed_dim,\n            image_embedding_size=(image_embedding_size, image_embedding_size),\n            input_image_size=(image_size, image_size),\n            mask_in_chans=16,\n        ),\n        mask_decoder=MaskDecoder(\n            num_multimask_outputs=3,\n            transformer=TwoWayTransformer(\n                depth=2,\n                embedding_dim=prompt_embed_dim,\n                mlp_dim=2048,\n                num_heads=8,\n            ),\n            transformer_dim=prompt_embed_dim,\n            iou_head_depth=3,\n            iou_head_hidden_dim=256,\n        ),\n        pixel_mean=[123.675, 116.28, 103.53],\n        pixel_std=[58.395, 57.12, 57.375],\n    )\n    sam.eval()\n    checkpoint = Path(checkpoint)\n    if checkpoint.name == \"sam_vit_b_01ec64.pth\" and not checkpoint.exists():\n        cmd = input(\"Download sam_vit_b_01ec64.pth from facebook AI? [y]/n: \")\n        if len(cmd) == 0 or cmd.lower() == \"y\":\n            checkpoint.parent.mkdir(parents=True, exist_ok=True)\n            print(\"Downloading SAM ViT-B checkpoint...\")\n            urllib.request.urlretrieve(\n                \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n                checkpoint,\n            )\n            print(checkpoint.name, \" is downloaded!\")\n    elif checkpoint.name == \"sam_vit_h_4b8939.pth\" and not checkpoint.exists():\n        cmd = input(\"Download sam_vit_h_4b8939.pth from facebook AI? [y]/n: \")\n        if len(cmd) == 0 or cmd.lower() == \"y\":\n            checkpoint.parent.mkdir(parents=True, exist_ok=True)\n            print(\"Downloading SAM ViT-H checkpoint...\")\n            urllib.request.urlretrieve(\n                \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n                checkpoint,\n            )\n            print(checkpoint.name, \" is downloaded!\")\n    elif checkpoint.name == \"sam_vit_l_0b3195.pth\" and not checkpoint.exists():\n        cmd = input(\"Download sam_vit_l_0b3195.pth from facebook AI? [y]/n: \")\n        if len(cmd) == 0 or cmd.lower() == \"y\":\n            checkpoint.parent.mkdir(parents=True, exist_ok=True)\n            print(\"Downloading SAM ViT-L checkpoint...\")\n            urllib.request.urlretrieve(\n                \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n                checkpoint,\n            )\n            print(checkpoint.name, \" is downloaded!\")\n\n    if checkpoint is not None:\n        with open(checkpoint, \"rb\") as f:\n            state_dict = torch.load(f, map_location=torch.device('cpu'))\n        sam.load_state_dict(state_dict)\n    return sam","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:37.293724Z","iopub.execute_input":"2024-04-17T04:31:37.294099Z","iopub.status.idle":"2024-04-17T04:31:37.310647Z","shell.execute_reply.started":"2024-04-17T04:31:37.294070Z","shell.execute_reply":"2024-04-17T04:31:37.309490Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def build_sam_vit_b(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=768,\n        encoder_depth=12,\n        encoder_num_heads=12,\n        encoder_global_attn_indexes=[2, 5, 8, 11],\n        checkpoint=checkpoint,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:38.741177Z","iopub.execute_input":"2024-04-17T04:31:38.741729Z","iopub.status.idle":"2024-04-17T04:31:38.750584Z","shell.execute_reply.started":"2024-04-17T04:31:38.741681Z","shell.execute_reply":"2024-04-17T04:31:38.749479Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda\"","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:39.746427Z","iopub.execute_input":"2024-04-17T04:31:39.747306Z","iopub.status.idle":"2024-04-17T04:31:39.751911Z","shell.execute_reply.started":"2024-04-17T04:31:39.747262Z","shell.execute_reply":"2024-04-17T04:31:39.750804Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"m = torch.load(MedSAM_CKPT_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:32:41.559593Z","iopub.execute_input":"2024-04-17T04:32:41.560020Z","iopub.status.idle":"2024-04-17T04:32:41.992778Z","shell.execute_reply.started":"2024-04-17T04:32:41.559986Z","shell.execute_reply":"2024-04-17T04:32:41.991902Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"missing_keys = [\"mask_decoder.transformer.layers.0.self_attn.qkv.weight\", \"mask_decoder.transformer.layers.0.self_attn.qkv.bias\", \"mask_decoder.transformer.layers.0.self_attn.proj.weight\", \"mask_decoder.transformer.layers.0.self_attn.proj.bias\", \"mask_decoder.transformer.layers.1.self_attn.qkv.weight\", \"mask_decoder.transformer.layers.1.self_attn.qkv.bias\", \"mask_decoder.transformer.layers.1.self_attn.proj.weight\", \"mask_decoder.transformer.layers.1.self_attn.proj.bias\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:38:36.456588Z","iopub.execute_input":"2024-04-17T04:38:36.457343Z","iopub.status.idle":"2024-04-17T04:38:36.462496Z","shell.execute_reply.started":"2024-04-17T04:38:36.457303Z","shell.execute_reply":"2024-04-17T04:38:36.461408Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"m['mask_decoder.transformer.layers.0.self_attn.q_proj.weight']","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:37:55.469613Z","iopub.execute_input":"2024-04-17T04:37:55.470341Z","iopub.status.idle":"2024-04-17T04:37:55.481990Z","shell.execute_reply.started":"2024-04-17T04:37:55.470302Z","shell.execute_reply":"2024-04-17T04:37:55.480848Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0112, -0.0501,  0.0012,  ..., -0.0119,  0.0884,  0.0031],\n        [ 0.0083, -0.0704,  0.0654,  ...,  0.0258, -0.0625,  0.0054],\n        [-0.0119,  0.0378,  0.0483,  ..., -0.0214,  0.0199, -0.0372],\n        ...,\n        [-0.0012,  0.0217, -0.0135,  ...,  0.0172,  0.0024,  0.0173],\n        [ 0.0518,  0.0189,  0.0356,  ..., -0.2363, -0.1029,  0.0661],\n        [ 0.0252, -0.0123,  0.0357,  ..., -0.0411,  0.0496, -0.0607]],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"m['mask_decoder.transformer.layers.0.self_attn.q_proj.bias']","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:39:14.027305Z","iopub.execute_input":"2024-04-17T04:39:14.028250Z","iopub.status.idle":"2024-04-17T04:39:14.043328Z","shell.execute_reply.started":"2024-04-17T04:39:14.028213Z","shell.execute_reply":"2024-04-17T04:39:14.042485Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor([ 4.8313e-02, -6.5631e-02, -2.8008e-02, -8.4526e-02,  5.4997e-03,\n        -1.6651e-03, -2.1064e-02, -3.5386e-02, -5.3049e-02,  1.3384e-01,\n         6.7545e-02,  2.3547e-02,  4.9364e-03,  1.8661e-02, -4.6092e-02,\n        -4.8251e-02,  2.1159e-02, -1.4857e-02,  8.1134e-02, -4.0158e-02,\n        -6.3415e-02, -1.1877e-01, -1.1133e-01,  8.2098e-02, -1.0438e-01,\n        -9.3557e-02, -2.9196e-02, -1.1532e-02, -1.1881e-01, -1.1254e-02,\n        -2.5300e-03,  1.8061e-01, -1.4535e-01,  4.8864e-02,  1.6383e-02,\n         9.0867e-02,  1.7463e-01, -1.0900e-01,  1.5575e-01, -1.6515e-01,\n        -3.7097e-04,  1.3114e-01,  3.8044e-02, -9.7976e-02,  6.7273e-03,\n         2.8579e-03,  2.5273e-03, -4.2528e-02, -2.2757e-01, -2.3179e-02,\n        -8.9680e-03,  5.5694e-02, -2.8435e-01,  8.0633e-02, -2.3394e-04,\n        -1.5514e-02, -2.2577e-01,  1.8180e-01, -4.9193e-02, -4.0370e-02,\n        -1.7515e-01, -1.0930e-01, -2.7282e-02, -1.5113e-01,  4.6134e-02,\n        -7.1125e-02,  1.5982e-01, -4.0528e-02, -2.0670e-01, -3.4211e-02,\n        -1.2194e-02, -2.3679e-01, -1.0048e-01, -4.3299e-03,  1.5230e-01,\n         1.2595e-02,  1.3312e-01,  1.7167e-01, -6.3291e-02,  2.2746e-01,\n        -3.7320e-02,  2.9668e-02,  1.2228e-01,  7.6686e-02,  4.0380e-02,\n        -4.7546e-02,  1.0953e-01, -8.9401e-03, -1.6978e-01,  8.9439e-02,\n        -1.1205e-01,  1.7711e-02, -2.5560e-02,  1.4858e-01, -1.0795e-01,\n        -1.9707e-02, -1.0263e-01, -6.9956e-02, -2.3504e-01, -2.0824e-01,\n        -2.5655e-02, -4.0776e-02, -1.8512e-01,  1.8749e-02, -7.6606e-02,\n         2.2868e-01,  1.2970e-02,  2.0674e-01,  1.1252e-02, -3.5724e-02,\n        -2.5879e-02,  1.0955e-01, -7.0541e-02, -3.2235e-02, -5.7310e-02,\n         8.3071e-02,  4.4505e-02, -1.9782e-01,  5.3445e-02,  9.4230e-02,\n         2.9112e-03, -5.1382e-02,  2.5000e-02, -1.0675e-02, -1.9159e-01,\n         1.2052e-02, -8.7870e-02, -1.1365e-01,  8.8679e-02,  1.1375e-01,\n         1.1774e-01, -1.0183e-01, -1.6562e-02,  9.8438e-02,  1.1818e-01,\n         4.7417e-02, -1.4583e-01,  9.0293e-02,  5.2547e-03,  1.4343e-01,\n         1.4638e-01, -8.2936e-02,  3.1263e-02,  1.4704e-01, -4.9213e-02,\n         5.8838e-02,  6.5891e-02,  7.1560e-02, -5.4358e-04,  1.4489e-02,\n         1.4284e-02,  2.0769e-02, -8.5309e-02,  8.1221e-02,  6.9080e-02,\n        -5.3046e-02,  4.2769e-02, -5.8639e-02,  1.2238e-01,  1.1338e-01,\n        -1.1455e-01, -1.0522e-01, -8.4732e-02,  1.1230e-01,  9.3641e-02,\n         4.2517e-02, -5.4360e-02,  3.6650e-02, -6.1037e-03, -3.3768e-02,\n         7.4309e-02,  1.7854e-02, -8.1363e-03,  5.1883e-02,  9.6179e-02,\n        -9.5506e-02,  6.1558e-02,  2.5143e-02,  9.4429e-02, -9.5089e-02,\n         6.8033e-02,  1.0302e-01, -1.2166e-01,  5.4661e-02,  3.4619e-02,\n        -1.7644e-02, -7.3936e-03, -1.1441e-02,  7.4280e-02,  1.3582e-02,\n         1.1738e-01,  3.3157e-02,  1.7425e-02, -1.5367e-02, -3.5325e-02,\n        -1.8435e-02,  1.9093e-02, -1.4689e-02, -3.7552e-02, -5.2393e-03,\n        -3.0926e-02,  2.4961e-02,  3.9334e-05,  3.2568e-02,  3.0182e-02,\n         3.3530e-02, -1.4363e-02,  2.4507e-03, -2.9095e-02,  4.7004e-02,\n         3.8140e-02,  4.8428e-03,  3.3648e-02, -1.6268e-02,  2.3752e-02,\n        -2.3373e-02, -1.2673e-02, -1.4885e-02, -4.8343e-02, -1.9888e-02,\n        -1.6334e-02, -3.8007e-02, -2.4658e-02, -1.5816e-02,  3.3233e-02,\n        -5.5612e-03,  1.4191e-01, -1.6801e-01, -5.0499e-02,  7.3405e-03,\n        -1.1778e-02,  4.1020e-02, -7.7988e-02,  1.5936e-01, -2.0031e-02,\n         1.8203e-02, -1.0238e-02, -5.6153e-03,  1.2883e-02, -1.9342e-01,\n        -7.6511e-03, -1.1143e-01, -1.1690e-02, -9.7641e-02, -6.0699e-02,\n        -4.8196e-02, -2.2757e-02,  1.1467e-01, -9.6931e-03, -9.1661e-04,\n        -1.2907e-02, -5.9586e-02, -2.1564e-02, -2.2723e-02, -2.0059e-01,\n         6.5411e-04], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"mod = build_sam_vit_b(MedSAM_CKPT_PATH)\nmedsam_model = mod.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:43:53.558117Z","iopub.execute_input":"2024-04-17T04:43:53.558865Z","iopub.status.idle":"2024-04-17T04:43:54.947025Z","shell.execute_reply.started":"2024-04-17T04:43:53.558826Z","shell.execute_reply":"2024-04-17T04:43:54.945871Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n# medsam_model = medsam_model.to(device)\n# medsam_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Medsam inference code\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n    if len(box_torch.shape) == 2:\n        box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    \n#     print(f'img_embed: {img_embed.shape} should be (B, 256, 64, 64)')\n#     print(f'sparse_embeddings : {sparse_embeddings.shape} (B, 2, 256)')\n    #print(f'src shape : {torch.repeat_interleave(img_embed, tokens.shape[0], dim=0)}')\n    print(f'dense_prompt_embeddings : {dense_embeddings.shape} (B, 256, 64, 64)')\n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:44:00.988343Z","iopub.execute_input":"2024-04-17T04:44:00.989594Z","iopub.status.idle":"2024-04-17T04:44:00.999406Z","shell.execute_reply.started":"2024-04-17T04:44:00.989543Z","shell.execute_reply":"2024-04-17T04:44:00.998426Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"valid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice().to(device)\njaccard_index_metric = BinaryJaccardIndex().to(device)\nwith torch.no_grad():\n    H = 240\n    W = 240\n    for i, batch in enumerate(tqdm(train_dataloader)):\n        # get image and masks from dataloader\n        imgs = batch[0].to(device)\n        gts = batch[1].to(device)\n        gts = gts.squeeze(dim=1)\n        bbs = batch[2].to(device)\n        \n        print(f'imgs.shape : {imgs.shape}')\n        print(f'gts.shape : {gts.shape}')\n        print(f'bbs.shape : {bbs.shape}')\n\n        with torch.no_grad():\n            image_embedding = medsam_model.image_encoder(imgs) # (1, 256, 64, 64)\n            #img_masks = img_masks.int()\n        y_pred = medsam_inference(medsam_model, image_embedding, bbs, H, W)\n        y_pred = torch.from_numpy(y_pred)\n        y_pred = y_pred.to(device)\n\n        # img_masks = img_masks[0][0].int()\n\n        batch_dice_score = dice_metric(y_pred, gts)\n        valid_batch_dice.append(batch_dice_score.to('cpu'))\n\n        batch_jaccard_score = jaccard_index_metric(y_pred, gts)\n        valid_batch_jaccard.append(batch_jaccard_score.to('cpu'))\n\n        print(f'DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')\n    print(f'Validation DICE score: {np.mean(valid_batch_dice)}, Validation Jaccard score: {np.mean(valid_batch_jaccard)}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:45:48.220782Z","iopub.execute_input":"2024-04-17T04:45:48.221200Z","iopub.status.idle":"2024-04-17T04:45:58.520656Z","shell.execute_reply.started":"2024-04-17T04:45:48.221166Z","shell.execute_reply":"2024-04-17T04:45:58.519049Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14338 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"830b7de9eccb44f7b3a9f745c0e82cc8"}},"metadata":{}},{"name":"stdout","text":"imgs.shape : torch.Size([4, 3, 1024, 1024])\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\ndense_prompt_embeddings : torch.Size([4, 256, 64, 64]) (B, 256, 64, 64)\nimgs.shape : torch.Size([4, 3, 1024, 1024])re: 0.0\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\ndense_prompt_embeddings : torch.Size([4, 256, 64, 64]) (B, 256, 64, 64)\nimgs.shape : torch.Size([4, 3, 1024, 1024])re: 0.0\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\ndense_prompt_embeddings : torch.Size([4, 256, 64, 64]) (B, 256, 64, 64)\nimgs.shape : torch.Size([4, 3, 1024, 1024])re: 0.0\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\ndense_prompt_embeddings : torch.Size([4, 256, 64, 64]) (B, 256, 64, 64)\nimgs.shape : torch.Size([4, 3, 1024, 1024])re: 0.0\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\ndense_prompt_embeddings : torch.Size([4, 256, 64, 64]) (B, 256, 64, 64)\nimgs.shape : torch.Size([4, 3, 1024, 1024])re: 0.0\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\ndense_prompt_embeddings : torch.Size([4, 256, 64, 64]) (B, 256, 64, 64)\nimgs.shape : torch.Size([4, 3, 1024, 1024])re: 0.0\ngts.shape : torch.Size([4, 240, 240])\nbbs.shape : torch.Size([4, 4])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbs.shape : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     image_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmedsam_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, 256, 64, 64)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#img_masks = img_masks.int()\u001b[39;00m\n\u001b[1;32m     22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m medsam_inference(medsam_model, image_embedding, bbs, H, W)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 98\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 98\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 162\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    160\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 162\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 224\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 224\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m     (attn \u001b[38;5;241m@\u001b[39m v)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    234\u001b[0m )\n","Cell \u001b[0;32mIn[8], line 356\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    354\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[1;32m    355\u001b[0m Rh \u001b[38;5;241m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[0;32m--> 356\u001b[0m Rw \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    359\u001b[0m r_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, q_h, q_w, dim)\n","Cell \u001b[0;32mIn[8], line 324\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    321\u001b[0m     rel_pos_resized \u001b[38;5;241m=\u001b[39m rel_pos\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Scale the coords with short length if shapes for q and k are different.\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m q_coords \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(q_size)[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m k_coords \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(k_size)[\u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size \u001b[38;5;241m/\u001b[39m k_size, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    326\u001b[0m relative_coords \u001b[38;5;241m=\u001b[39m (q_coords \u001b[38;5;241m-\u001b[39m k_coords) \u001b[38;5;241m+\u001b[39m (k_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size \u001b[38;5;241m/\u001b[39m k_size, \u001b[38;5;241m1.0\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"j = [i.to('cpu') for i in valid_batch_jaccard]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(j)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j = [i.to('cpu') for i in valid_batch_dice]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(j)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}