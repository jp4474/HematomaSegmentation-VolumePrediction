{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8086534,"sourceType":"datasetVersion","datasetId":4773550}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import libraries, import medsam checkpoint and define H, W, batch size","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nimport nibabel as nib\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\nfrom torchmetrics.classification import BinaryJaccardIndex, Dice\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchvision\n\n# %% environment and functions\nimport matplotlib.pyplot as plt\njoin = os.path.join\nfrom segment_anything import sam_model_registry\nfrom skimage import io, transform\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:06.831830Z","iopub.execute_input":"2024-04-15T17:15:06.832786Z","iopub.status.idle":"2024-04-15T17:15:17.497628Z","shell.execute_reply.started":"2024-04-15T17:15:06.832748Z","shell.execute_reply":"2024-04-15T17:15:17.496687Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"H = 240\nW = 240\nBATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:17.499594Z","iopub.execute_input":"2024-04-15T17:15:17.500208Z","iopub.status.idle":"2024-04-15T17:15:17.505420Z","shell.execute_reply.started":"2024-04-15T17:15:17.500178Z","shell.execute_reply":"2024-04-15T17:15:17.504574Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#%% load model and image\nMedSAM_CKPT_PATH = \"/kaggle/working/medsam_vit_b.pth\"\ndevice = \"cuda:0\"\nmedsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\nmedsam_model = medsam_model.to(device)\nmedsam_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:30.652903Z","iopub.execute_input":"2024-04-15T17:17:30.653532Z","iopub.status.idle":"2024-04-15T17:17:32.226175Z","shell.execute_reply.started":"2024-04-15T17:17:30.653499Z","shell.execute_reply":"2024-04-15T17:17:32.225297Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# !wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:32.337576Z","iopub.execute_input":"2024-04-15T17:15:32.338482Z","iopub.status.idle":"2024-04-15T17:17:24.917569Z","shell.execute_reply.started":"2024-04-15T17:15:32.338445Z","shell.execute_reply":"2024-04-15T17:17:24.916470Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-04-15 17:15:33--  https://zenodo.org/records/10689643/files/medsam_vit_b.pth\nResolving zenodo.org (zenodo.org)... 188.184.103.159, 188.185.79.172, 188.184.98.238, ...\nConnecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 375049145 (358M) [application/octet-stream]\nSaving to: 'medsam_vit_b.pth'\n\nmedsam_vit_b.pth    100%[===================>] 357.67M  14.4MB/s    in 1m 51s  \n\n2024-04-15 17:17:24 (3.23 MB/s) - 'medsam_vit_b.pth' saved [375049145/375049145]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Define Dataset Class, load data","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):    \n    def __init__(self, data_root_folder, folder = '', n_sample=None):\n        main_folder = os.path.join(data_root_folder, folder)\n        self.folder_path = os.path.join(main_folder, 'slice')\n\n    def __getitem__(self, index):\n        file_name = os.listdir(self.folder_path)[index]\n        sample = torch.from_numpy(np.load(os.path.join(self.folder_path, file_name)))\n        img_as_tensor = np.expand_dims(sample[0,:,:], axis=0)\n        mask_as_tensor = np.expand_dims(sample[1,:,:], axis=0)\n        return {\n            'image': img_as_tensor,\n            'mask': mask_as_tensor,\n            'img_id': file_name\n        }\n \n    def __len__(self):\n        return len(os.listdir(self.folder_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:38.255446Z","iopub.execute_input":"2024-04-15T17:17:38.256129Z","iopub.status.idle":"2024-04-15T17:17:38.263520Z","shell.execute_reply.started":"2024-04-15T17:17:38.256094Z","shell.execute_reply":"2024-04-15T17:17:38.262478Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"data_root_folder = '/kaggle/input/full_raw - Copy'\ntrain_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'train')\nval_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'val')\ntest_dataset = BraTSDataset(data_root_folder = data_root_folder, folder = 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:40.450257Z","iopub.execute_input":"2024-04-15T17:17:40.450622Z","iopub.status.idle":"2024-04-15T17:17:40.455843Z","shell.execute_reply.started":"2024-04-15T17:17:40.450589Z","shell.execute_reply":"2024-04-15T17:17:40.454832Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:41.855323Z","iopub.execute_input":"2024-04-15T17:17:41.856134Z","iopub.status.idle":"2024-04-15T17:17:41.860306Z","shell.execute_reply.started":"2024-04-15T17:17:41.856101Z","shell.execute_reply":"2024-04-15T17:17:41.859211Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:42.957245Z","iopub.execute_input":"2024-04-15T17:17:42.958064Z","iopub.status.idle":"2024-04-15T17:17:48.675465Z","shell.execute_reply.started":"2024-04-15T17:17:42.958032Z","shell.execute_reply":"2024-04-15T17:17:48.674468Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 3. Define medsam, mask and bounding box functions","metadata":{}},{"cell_type":"markdown","source":"## Self-defined functions","metadata":{}},{"cell_type":"code","source":"def convert_to_binary(arr):\n    return np.where(arr > 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:50.533598Z","iopub.execute_input":"2024-04-15T17:17:50.534453Z","iopub.status.idle":"2024-04-15T17:17:50.538677Z","shell.execute_reply.started":"2024-04-15T17:17:50.534418Z","shell.execute_reply":"2024-04-15T17:17:50.537715Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def visualize_masks(nii_img):\n    nii_aff  = nii_img.affine\n    nii_hdr  = nii_img.header\n\n    # print(nii_aff ,'\\n',nii_hdr)\n    # print(nii_data.shape)\n    nii_data = nii_img.get_fdata()\n    number_of_slices = nii_data.shape[1]\n    number_of_frames = nii_data.shape[2]\n    number_of_frames = 1\n\n    # Define the number of columns for subplot\n    num_columns = 5\n\n    if(len(nii_data.shape)==3):\n        start_slice = 80\n        end_slice = 100\n        num_slices = end_slice - start_slice + 1\n        num_rows = num_slices // num_columns + 1\n\n        fig, ax = plt.subplots(num_rows, num_columns, figsize=(10,10))\n\n        for i in range(start_slice, end_slice+1):\n            row = (i-start_slice) // num_columns\n            col = (i-start_slice) % num_columns\n            ax[row, col].imshow(nii_data[:,:,i])\n            ax[row, col].axis('off')\n\n        # Remove empty subplots\n        for j in range(i-start_slice+1, num_rows*num_columns):\n            fig.delaxes(ax.flatten()[j])\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:52.609283Z","iopub.execute_input":"2024-04-15T17:17:52.610138Z","iopub.status.idle":"2024-04-15T17:17:52.618748Z","shell.execute_reply.started":"2024-04-15T17:17:52.610106Z","shell.execute_reply":"2024-04-15T17:17:52.617672Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask_batch(batch_mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    counter = 0\n    box_1024_batch = []\n    useless_image_indices = []\n    for ind, mask in enumerate(batch_mask):\n#         print(\"mask shape\", mask.shape)\n        mask = mask[0]\n#         print(\"mask shape 2\", mask.shape)\n        nonzero_indices = torch.nonzero(mask).T\n#         print(\"nonzero_indices shape:\", nonzero_indices.shape)\n#         nonzero_indices = torch.transpose(torch.nonzero(mask).T)\n        if nonzero_indices.shape[-1] == 0:\n            useless_image_indices.append(ind)\n            box_1024 = torch.asarray([[-1, -1, -1, -1]])\n#             x0 = 0\n#             y0 = 0\n#             x1 = \n        else:\n            min_y = torch.min(nonzero_indices[0, :])\n            max_y = torch.max(nonzero_indices[0, :])\n            min_x = torch.min(nonzero_indices[1, :])\n            max_x = torch.max(nonzero_indices[1, :])\n\n            margin = 5\n\n            x0 = min_x - margin\n            y0 = min_y - margin\n            x1 = max_x + margin\n            y1 = max_y + margin\n\n            # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n            # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n\n            box_np = torch.asarray([[x0,y0, x1, y1]])\n            box_1024 = box_np / torch.asarray([W, H, W, H]) * 1024\n#         print(\"box 1024 shape\", box_1024.shape)\n        box_1024_batch.append(box_1024)\n\n    if counter == BATCH_SIZE:\n        return         \n        \n    final_tensor = torch.cat(box_1024_batch, dim = 0)\n    final_tensor = final_tensor.unsqueeze(dim = 1)\n\n#     print(final_tensor.shape)\n    return final_tensor, useless_image_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:54.702421Z","iopub.execute_input":"2024-04-15T17:17:54.702800Z","iopub.status.idle":"2024-04-15T17:17:54.712930Z","shell.execute_reply.started":"2024-04-15T17:17:54.702771Z","shell.execute_reply":"2024-04-15T17:17:54.712057Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## MedSAM functions","metadata":{}},{"cell_type":"code","source":"# visualization functions\n# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n# change color to avoid red and green\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([251/255, 252/255, 30/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    # print(\"x_0, y_0\", x0, y0)\n    # print(\"w, h\", w, h)\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:56.886863Z","iopub.execute_input":"2024-04-15T17:17:56.887211Z","iopub.status.idle":"2024-04-15T17:17:56.895684Z","shell.execute_reply.started":"2024-04-15T17:17:56.887184Z","shell.execute_reply":"2024-04-15T17:17:56.894749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def generate_box_and_embedding(img, img_mask):\n    H = 240\n    W = 240\n\n    #%% image preprocessing part 2\n    resizer = torchvision.transforms.Resize(size = (1024,1024))\n    img_1024 = resizer(img)\n#     print(\"img_1024 shape: \", img_1024.shape)\n\n    # generate boxes and useless image indices\n    box_1024, useless_image_indices = box_coordinates_from_mask_batch(img_mask)\n#     print(img_1024.shape)\n\n    # generate negated splicer\n    useful_indices = torch.ones(BATCH_SIZE, dtype=bool)\n    for index in useless_image_indices:\n        useful_indices[index] = False\n#     negated_splicer = ~splicer\n    \n#     img_1024 = img_1024.repeat(1, 3, 1, 1)\n    \n    with torch.no_grad():\n        image_embedding = medsam_model.image_encoder(img_1024) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n    return image_embedding, box_1024, useful_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:17:59.010077Z","iopub.execute_input":"2024-04-15T17:17:59.010430Z","iopub.status.idle":"2024-04-15T17:17:59.017258Z","shell.execute_reply.started":"2024-04-15T17:17:59.010401Z","shell.execute_reply":"2024-04-15T17:17:59.016371Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test = torch.Tensor([1,2,3])\ntest[[False,False, True]]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:18:01.747791Z","iopub.execute_input":"2024-04-15T17:18:01.748449Z","iopub.status.idle":"2024-04-15T17:18:01.783789Z","shell.execute_reply.started":"2024-04-15T17:18:01.748417Z","shell.execute_reply":"2024-04-15T17:18:01.782919Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([3.])"},"metadata":{}}]},{"cell_type":"code","source":"# Medsam inference code\n@torch.no_grad()\ndef medsam_inference(medsam_model, img_embed, box_1024, H, W):\n    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n#     if len(box_torch.shape) == 2:\n#         box_torch = box_torch[:, None, :] # (B, 1, 4)\n\n    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n        points=None,\n        boxes=box_torch,\n        masks=None,\n    )\n    \n    low_res_logits, _ = medsam_model.mask_decoder(\n        image_embeddings=img_embed, # (B, 256, 64, 64)\n        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n        multimask_output=False,\n        )\n\n    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n\n    low_res_pred = F.interpolate(\n        low_res_pred,\n        size=(H, W),\n        mode=\"bilinear\",\n        align_corners=False,\n    )  # (1, 1, gt.shape)\n    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n    return medsam_seg","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:18:05.703193Z","iopub.execute_input":"2024-04-15T17:18:05.703907Z","iopub.status.idle":"2024-04-15T17:18:05.711478Z","shell.execute_reply.started":"2024-04-15T17:18:05.703873Z","shell.execute_reply":"2024-04-15T17:18:05.710498Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training loop","metadata":{}},{"cell_type":"code","source":"def all_false(lst):\n    return all(element == False for element in lst)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:18:07.978282Z","iopub.execute_input":"2024-04-15T17:18:07.978617Z","iopub.status.idle":"2024-04-15T17:18:07.983032Z","shell.execute_reply.started":"2024-04-15T17:18:07.978591Z","shell.execute_reply":"2024-04-15T17:18:07.982082Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(len(val_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.851922Z","iopub.status.idle":"2024-04-15T17:15:18.852355Z","shell.execute_reply.started":"2024-04-15T17:15:18.852125Z","shell.execute_reply":"2024-04-15T17:15:18.852143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.853523Z","iopub.status.idle":"2024-04-15T17:15:18.853846Z","shell.execute_reply.started":"2024-04-15T17:15:18.853670Z","shell.execute_reply":"2024-04-15T17:15:18.853682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\n# Initialize metrics and metrics lists\nfrom time import time\nstart_time = time()\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\n# [B, C, 240, 240]\n# [C, 240, 240]\nwith torch.no_grad():\n    for i, batch in enumerate(tqdm(val_dataloader)):\n        print(\"Iteration nb: \", i)\n        if i%100 == 99:\n            print(\"Iteration nb: \", i)\n            print(\"Elapsed time:\", time() - start_time)\n#         print(\"Iteration \", i)\n        # get image and masks from dataloader, change nb channels = 3\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'] #.to(device).float()\n        imgs = imgs.repeat(1, 3, 1, 1)\n#         img_masks = img_masks.repeat(1, 3, 1, 1)\n        # get non-empty image embeddings, bounding boxes and masks (i.e. labels)\n        image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n        if all_false(useful_indices):\n            continue\n        useful_box_1024 = box_1024[useful_indices,:,:][0]\n        useful_image_embedding = image_embedding[useful_indices,:,:,:][0]\n        y_true = img_masks[useful_indices,:,:,:][0]\n        useful_imgs = imgs[useful_indices,:,:,:][0]\n        \n#         print(\"image embedding shape\", useful_image_embedding.shape)\n#         print(\"box shape\", useful_box_1024.shape)\n#         print(\"mask shape\", y_true.shape)\n\n        # generate predictions using medsam_inference\n#         print(medsam_pred.shape)\n        output = medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W)\n        y_pred = torch.from_numpy(output).unsqueeze(dim=0)\n#         y_pred = net(imgs).cpu()\n        \n#         print(\"preds shape\", y_pred.shape)\n#         print(\"mask shape\", y_true.shape)\n        #TODO remove\n#         y_true = y_true[0]\n        print(torch.unique(y_pred))\n        print(torch.unique(y_true))\n        \n        batch_dice_score = dice_metric(y_pred, y_true)\n        print(\"Dice\", batch_dice_score)\n        valid_batch_dice.append(batch_dice_score)\n    \n        batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n        print(\"Jaccard\", batch_jaccard_score)\n        valid_batch_jaccard.append(batch_jaccard_score)\n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:55:38.172811Z","iopub.execute_input":"2024-04-15T17:55:38.173718Z","iopub.status.idle":"2024-04-15T17:56:10.227039Z","shell.execute_reply.started":"2024-04-15T17:55:38.173667Z","shell.execute_reply":"2024-04-15T17:56:10.225606Z"},"trusted":true},"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28985 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fec0877290242be8f89e62a94f5051c"}},"metadata":{}},{"name":"stdout","text":"Iteration nb:  0\nIteration nb:  1\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9900)\nJaccard tensor(0.5152)\nIteration nb:  2\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9832)\nJaccard tensor(0.5776)\nIteration nb:  3\nIteration nb:  4\nIteration nb:  5\nIteration nb:  6\nIteration nb:  7\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9803)\nJaccard tensor(0.6029)\nIteration nb:  8\nIteration nb:  9\nIteration nb:  10\nIteration nb:  11\nIteration nb:  12\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9928)\nJaccard tensor(0.5741)\nIteration nb:  13\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9616)\nJaccard tensor(0.5927)\nIteration nb:  14\nIteration nb:  15\nIteration nb:  16\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9688)\nJaccard tensor(0.4906)\nIteration nb:  17\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9898)\nJaccard tensor(0.5443)\nIteration nb:  18\nIteration nb:  19\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9925)\nJaccard tensor(0.7403)\nIteration nb:  20\nIteration nb:  21\nIteration nb:  22\nIteration nb:  23\nIteration nb:  24\nIteration nb:  25\nIteration nb:  26\nIteration nb:  27\nIteration nb:  28\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9953)\nJaccard tensor(0.5298)\nIteration nb:  29\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9882)\nJaccard tensor(0.7494)\nIteration nb:  30\nIteration nb:  31\nIteration nb:  32\nIteration nb:  33\nIteration nb:  34\nIteration nb:  35\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9943)\nJaccard tensor(0.5456)\nIteration nb:  36\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9807)\nJaccard tensor(0.5146)\nIteration nb:  37\nIteration nb:  38\nIteration nb:  39\nIteration nb:  40\nIteration nb:  41\nIteration nb:  42\nIteration nb:  43\nIteration nb:  44\nIteration nb:  45\nIteration nb:  46\nIteration nb:  47\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9883)\nJaccard tensor(0.6455)\nIteration nb:  48\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9947)\nJaccard tensor(0.6626)\nIteration nb:  49\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9846)\nJaccard tensor(0.6826)\nIteration nb:  50\nIteration nb:  51\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9780)\nJaccard tensor(0.6510)\nIteration nb:  52\nIteration nb:  53\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9873)\nJaccard tensor(0.4161)\nIteration nb:  54\nIteration nb:  55\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9918)\nJaccard tensor(0.7102)\nIteration nb:  56\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9610)\nJaccard tensor(0.2849)\nIteration nb:  57\nIteration nb:  58\nIteration nb:  59\nIteration nb:  60\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9822)\nJaccard tensor(0.5280)\nIteration nb:  61\nIteration nb:  62\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9981)\nJaccard tensor(0.5547)\nIteration nb:  63\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9885)\nJaccard tensor(0.1599)\nIteration nb:  64\nIteration nb:  65\nIteration nb:  66\nIteration nb:  67\nIteration nb:  68\nIteration nb:  69\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9922)\nJaccard tensor(0.6920)\nIteration nb:  70\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9755)\nJaccard tensor(0.6047)\nIteration nb:  71\nIteration nb:  72\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9946)\nJaccard tensor(0.6774)\nIteration nb:  73\nIteration nb:  74\nIteration nb:  75\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9582)\nJaccard tensor(0.6327)\nIteration nb:  76\nIteration nb:  77\nIteration nb:  78\nIteration nb:  79\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9831)\nJaccard tensor(0.6159)\nIteration nb:  80\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9684)\nJaccard tensor(0.6547)\nIteration nb:  81\nIteration nb:  82\nIteration nb:  83\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9885)\nJaccard tensor(0.6782)\nIteration nb:  84\nIteration nb:  85\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9941)\nJaccard tensor(0.5384)\nIteration nb:  86\nIteration nb:  87\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9903)\nJaccard tensor(0.6099)\nIteration nb:  88\nIteration nb:  89\nIteration nb:  90\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9842)\nJaccard tensor(0.7111)\nIteration nb:  91\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9927)\nJaccard tensor(0.4940)\nIteration nb:  92\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9971)\nJaccard tensor(0.5793)\nIteration nb:  93\nIteration nb:  94\nIteration nb:  95\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9882)\nJaccard tensor(0.4728)\nIteration nb:  96\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9928)\nJaccard tensor(0.0304)\nIteration nb:  97\nIteration nb:  98\nIteration nb:  99\nIteration nb:  99\nElapsed time: 24.22501039505005\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9680)\nJaccard tensor(0.5408)\nIteration nb:  100\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9919)\nJaccard tensor(0.7556)\nIteration nb:  101\nIteration nb:  102\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9760)\nJaccard tensor(0.4298)\nIteration nb:  103\nIteration nb:  104\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9973)\nJaccard tensor(0.6100)\nIteration nb:  105\nIteration nb:  106\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9903)\nJaccard tensor(0.6500)\nIteration nb:  107\nIteration nb:  108\nIteration nb:  109\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9946)\nJaccard tensor(0.6190)\nIteration nb:  110\nIteration nb:  111\nIteration nb:  112\nIteration nb:  113\nIteration nb:  114\nIteration nb:  115\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9791)\nJaccard tensor(0.6410)\nIteration nb:  116\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9703)\nJaccard tensor(0.4614)\nIteration nb:  117\nIteration nb:  118\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9925)\nJaccard tensor(0.2543)\nIteration nb:  119\nIteration nb:  120\nIteration nb:  121\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9812)\nJaccard tensor(0.6451)\nIteration nb:  122\nIteration nb:  123\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9448)\nJaccard tensor(0.3006)\nIteration nb:  124\nIteration nb:  125\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9789)\nJaccard tensor(0.5691)\nIteration nb:  126\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9825)\nJaccard tensor(0.4086)\nIteration nb:  127\nIteration nb:  128\ntensor([0, 1], dtype=torch.uint8)\ntensor([0, 1], dtype=torch.int16)\nDice tensor(0.9888)\nJaccard tensor(0.4633)\nIteration nb:  129\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[65], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         img_masks = img_masks.repeat(1, 3, 1, 1)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# get non-empty image embeddings, bounding boxes and masks (i.e. labels)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m         image_embedding, box_1024, useful_indices \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_box_and_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m all_false(useful_indices):\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n","Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mgenerate_box_and_embedding\u001b[0;34m(img, img_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     negated_splicer = ~splicer\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     img_1024 = img_1024.repeat(1, 3, 1, 1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m         image_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmedsam_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_1024\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, 256, 64, 64)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     print(\"image_embedding shape\", image_embedding.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     print(\"box_1024 shape\", box_1024.shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_embedding, box_1024, useful_indices\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Dice tensor(0.9882)\n# Jaccard tensor(0.7494)\n\n# Training loop\n# Initialize metrics and metrics lists\nfrom time import time\nstart_time = time()\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\n# [B, C, 240, 240]\n# [C, 240, 240]\nwith torch.no_grad():\n    for i, batch in enumerate(tqdm(val_dataloader)):\n        print(\"Iteration nb: \", i)\n        if i%100 == 99:\n            print(\"Iteration nb: \", i)\n            print(\"Elapsed time:\", time() - start_time)\n#         print(\"Iteration \", i)\n        # get image and masks from dataloader, change nb channels = 3\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'] #.to(device).float()\n        imgs = imgs.repeat(1, 3, 1, 1)\n#         img_masks = img_masks.repeat(1, 3, 1, 1)\n        # get non-empty image embeddings, bounding boxes and masks (i.e. labels)\n        image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n        if all_false(useful_indices):\n            continue\n        useful_box_1024 = box_1024[useful_indices,:,:][0]\n        useful_image_embedding = image_embedding[useful_indices,:,:,:][0]\n        y_true = img_masks[useful_indices,:,:,:][0]\n        useful_imgs = imgs[useful_indices,:,:,:][0]\n        \n#         print(\"image embedding shape\", useful_image_embedding.shape)\n#         print(\"box shape\", useful_box_1024.shape)\n#         print(\"mask shape\", y_true.shape)\n\n        # generate predictions using medsam_inference\n#         print(medsam_pred.shape)\n        output = medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W)\n        y_pred = torch.from_numpy(output).unsqueeze(dim=0)\n#         y_pred = net(imgs).cpu()\n        \n#         print(\"preds shape\", y_pred.shape)\n#         print(\"mask shape\", y_true.shape)\n        #TODO remove\n#         y_true = y_true[0]\n        \n        batch_dice_score = dice_metric(y_pred, y_true)\n        print(\"Dice\", batch_dice_score)\n        valid_batch_dice.append(batch_dice_score)\n    \n        batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n        print(\"Jaccard\", batch_jaccard_score)\n        valid_batch_jaccard.append(batch_jaccard_score)\n        if i == 29:\n            box_240 = useful_box_1024 / torch.asarray([1024, 1024, 1024, 1024]) * 240\n            fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n            ax[0].imshow(imgs[0][0,:,:].cpu())\n            show_box(box_240[0].numpy(), ax[0])\n            ax[0].set_title(\"Input Image and Bounding Box\")\n            ax[1].imshow(y_pred[0])\n            # show_mask(medsam_seg, ax[1])\n            # show_box(box_np[0], ax[1])\n            ax[1].set_title(\"MedSAM Segmentation\")\n\n            ax[2].imshow(y_true[0])\n            ax[2].set_title(\"Mask\")\n            plt.show()        \n            break            \n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:38:04.095137Z","iopub.execute_input":"2024-04-15T17:38:04.095521Z","iopub.status.idle":"2024-04-15T17:38:12.057393Z","shell.execute_reply.started":"2024-04-15T17:38:04.095491Z","shell.execute_reply":"2024-04-15T17:38:12.056401Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28985 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d462cd68b4843b1a4977fadb30b0236"}},"metadata":{}},{"name":"stdout","text":"Iteration nb:  0\nIteration nb:  1\nDice tensor(0.9900)\nJaccard tensor(0.5152)\nIteration nb:  2\nDice tensor(0.9832)\nJaccard tensor(0.5776)\nIteration nb:  3\nIteration nb:  4\nIteration nb:  5\nIteration nb:  6\nIteration nb:  7\nDice tensor(0.9803)\nJaccard tensor(0.6029)\nIteration nb:  8\nIteration nb:  9\nIteration nb:  10\nIteration nb:  11\nIteration nb:  12\nDice tensor(0.9928)\nJaccard tensor(0.5741)\nIteration nb:  13\nDice tensor(0.9616)\nJaccard tensor(0.5927)\nIteration nb:  14\nIteration nb:  15\nIteration nb:  16\nDice tensor(0.9688)\nJaccard tensor(0.4906)\nIteration nb:  17\nDice tensor(0.9898)\nJaccard tensor(0.5443)\nIteration nb:  18\nIteration nb:  19\nDice tensor(0.9925)\nJaccard tensor(0.7403)\nIteration nb:  20\nIteration nb:  21\nIteration nb:  22\nIteration nb:  23\nIteration nb:  24\nIteration nb:  25\nIteration nb:  26\nIteration nb:  27\nIteration nb:  28\nDice tensor(0.9953)\nJaccard tensor(0.5298)\nIteration nb:  29\nDice tensor(0.9882)\nJaccard tensor(0.7494)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 3 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# 16\n# Dice tensor(0.9688)\n# Jaccard tensor(0.4906)\n\n# Training loop\n# Initialize metrics and metrics lists\nfrom time import time\nstart_time = time()\nvalid_batch_dice = []\nvalid_batch_jaccard = []\ndice_metric = Dice()\njaccard_index_metric = BinaryJaccardIndex()\n# [B, C, 240, 240]\n# [C, 240, 240]\nwith torch.no_grad():\n    for i, batch in enumerate(tqdm(val_dataloader)):\n        print(\"Iteration nb: \", i)\n        if i%100 == 99:\n            print(\"Iteration nb: \", i)\n            print(\"Elapsed time:\", time() - start_time)\n#         print(\"Iteration \", i)\n        # get image and masks from dataloader, change nb channels = 3\n        imgs = batch['image'].to(device).float()\n        img_masks = batch['mask'] #.to(device).float()\n        imgs = imgs.repeat(1, 3, 1, 1)\n#         img_masks = img_masks.repeat(1, 3, 1, 1)\n        # get non-empty image embeddings, bounding boxes and masks (i.e. labels)\n        image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n        if all_false(useful_indices):\n            continue\n        useful_box_1024 = box_1024[useful_indices,:,:][0]\n        useful_image_embedding = image_embedding[useful_indices,:,:,:][0]\n        y_true = img_masks[useful_indices,:,:,:][0]\n        useful_imgs = imgs[useful_indices,:,:,:][0]\n        \n#         print(\"image embedding shape\", useful_image_embedding.shape)\n#         print(\"box shape\", useful_box_1024.shape)\n#         print(\"mask shape\", y_true.shape)\n\n        # generate predictions using medsam_inference\n#         print(medsam_pred.shape)\n        output = medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W)\n        y_pred = torch.from_numpy(output).unsqueeze(dim=0)\n        print(j_d(y_pred, y_true))\n#         y_pred = net(imgs).cpu()\n        \n#         print(\"preds shape\", y_pred.shape)\n#         print(\"mask shape\", y_true.shape)\n        #TODO remove\n#         y_true = y_true[0]\n        \n#         batch_dice_score = dice_metric(y_pred, y_true)\n#         intersect = x.intersection(y)\n\n#         print(\"Dice\", batch_dice_score)\n#         valid_batch_dice.append(batch_dice_score)\n    \n#         batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n#         print(\"Jaccard\", batch_jaccard_score)\n          \n#         valid_batch_jaccard.append(batch_jaccard_score)\n#         if i == 16:\n#             box_240 = useful_box_1024 / torch.asarray([1024, 1024, 1024, 1024]) * 240\n#             fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n#             ax[0].imshow(imgs[0][0,:,:].cpu())\n#             show_box(box_240[0].numpy(), ax[0])\n#             ax[0].set_title(\"Input Image and Bounding Box\")\n#             ax[1].imshow(y_pred[0])\n#             # show_mask(medsam_seg, ax[1])\n#             # show_box(box_np[0], ax[1])\n#             ax[1].set_title(\"MedSAM Segmentation\")\n\n#             ax[2].imshow(y_true[0])\n#             ax[2].set_title(\"Mask\")\n#             plt.show()        \n#             break            \n\n#         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:51:55.774134Z","iopub.execute_input":"2024-04-15T17:51:55.774892Z","iopub.status.idle":"2024-04-15T17:52:19.790383Z","shell.execute_reply.started":"2024-04-15T17:51:55.774857Z","shell.execute_reply":"2024-04-15T17:52:19.789063Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28985 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa0240dbf57d431a867fd2302d324cfe"}},"metadata":{}},{"name":"stdout","text":"Iteration nb:  0\nIteration nb:  1\n(0.9801980257034302, 0.9801980257034302)\nIteration nb:  2\n(0.9669108390808105, 0.9669107794761658)\nIteration nb:  3\nIteration nb:  4\nIteration nb:  5\nIteration nb:  6\nIteration nb:  7\n(0.9613852500915527, 0.9613853096961975)\nIteration nb:  8\nIteration nb:  9\nIteration nb:  10\nIteration nb:  11\nIteration nb:  12\n(0.9857276082038879, 0.9857276082038879)\nIteration nb:  13\n(0.9259704947471619, 0.9259705543518066)\nIteration nb:  14\nIteration nb:  15\nIteration nb:  16\n(0.9395245313644409, 0.9395244717597961)\nIteration nb:  17\n(0.9798576831817627, 0.9798577427864075)\nIteration nb:  18\nIteration nb:  19\n(0.985077440738678, 0.9850775003433228)\nIteration nb:  20\nIteration nb:  21\nIteration nb:  22\nIteration nb:  23\nIteration nb:  24\nIteration nb:  25\nIteration nb:  26\nIteration nb:  27\nIteration nb:  28\n(0.9907375574111938, 0.9907375574111938)\nIteration nb:  29\n(0.9767661094665527, 0.976766049861908)\nIteration nb:  30\nIteration nb:  31\nIteration nb:  32\nIteration nb:  33\nIteration nb:  34\nIteration nb:  35\n(0.9886412620544434, 0.9886412620544434)\nIteration nb:  36\n(0.9620533585548401, 0.9620533585548401)\nIteration nb:  37\nIteration nb:  38\nIteration nb:  39\nIteration nb:  40\nIteration nb:  41\nIteration nb:  42\nIteration nb:  43\nIteration nb:  44\nIteration nb:  45\nIteration nb:  46\nIteration nb:  47\n(0.9768000841140747, 0.9768000245094299)\nIteration nb:  48\n(0.9895341992378235, 0.9895341992378235)\nIteration nb:  49\n(0.9696347713470459, 0.9696348309516907)\nIteration nb:  50\nIteration nb:  51\n(0.9569870829582214, 0.9569871425628662)\nIteration nb:  52\nIteration nb:  53\n(0.9749361276626587, 0.9749361276626587)\nIteration nb:  54\nIteration nb:  55\n(0.9837101697921753, 0.9837101101875305)\nIteration nb:  56\n(0.9249085187911987, 0.9249085187911987)\nIteration nb:  57\nIteration nb:  58\nIteration nb:  59\nIteration nb:  60\n(0.9649649262428284, 0.9649649262428284)\nIteration nb:  61\nIteration nb:  62\n(0.9961878061294556, 0.9961878061294556)\nIteration nb:  63\n(0.9772750735282898, 0.9772751927375793)\nIteration nb:  64\nIteration nb:  65\nIteration nb:  66\nIteration nb:  67\nIteration nb:  68\nIteration nb:  69\n(0.9845303297042847, 0.9845303297042847)\nIteration nb:  70\n(0.9522445797920227, 0.9522446393966675)\nIteration nb:  71\nIteration nb:  72\n(0.9892937541007996, 0.9892937541007996)\nIteration nb:  73\nIteration nb:  74\nIteration nb:  75\n(0.9197440147399902, 0.919744074344635)\nIteration nb:  76\nIteration nb:  77\nIteration nb:  78\nIteration nb:  79\n(0.9667764902114868, 0.966776430606842)\nIteration nb:  80\n(0.9386432766914368, 0.9386432766914368)\nIteration nb:  81\nIteration nb:  82\nIteration nb:  83\n(0.9772750735282898, 0.9772751927375793)\nIteration nb:  84\nIteration nb:  85\n(0.9883666634559631, 0.9883667230606079)\nIteration nb:  86\nIteration nb:  87\n(0.9807087182998657, 0.980708658695221)\nIteration nb:  88\nIteration nb:  89\nIteration nb:  90\n(0.9688605666160583, 0.9688605666160583)\nIteration nb:  91\n(0.9855564832687378, 0.9855563640594482)\nIteration nb:  92\n(0.9942181706428528, 0.9942181706428528)\nIteration nb:  93\nIteration nb:  94\nIteration nb:  95\n(0.9767321944236755, 0.9767321944236755)\nIteration nb:  96\n(0.9856933355331421, 0.9856932759284973)\nIteration nb:  97\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#         img_masks = img_masks.repeat(1, 3, 1, 1)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# get non-empty image embeddings, bounding boxes and masks (i.e. labels)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         image_embedding, box_1024, useful_indices \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_box_and_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m all_false(useful_indices):\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n","Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mgenerate_box_and_embedding\u001b[0;34m(img, img_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     negated_splicer = ~splicer\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     img_1024 = img_1024.repeat(1, 3, 1, 1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m         image_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmedsam_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_1024\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, 256, 64, 64)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     print(\"image_embedding shape\", image_embedding.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     print(\"box_1024 shape\", box_1024.shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_embedding, box_1024, useful_indices\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def j_d(y_pred, y_true):\n    num_intersection = (torch.sum(y_pred == y_true))\n    total_num = 240 * 240\n    j = num_intersection/(total_num * 2 - num_intersection)\n    d = (2 * num_intersection)/(total_num * 2)\n    perhaps_j = d/(2-d)\n    return j.item(), perhaps_j.item()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:47:50.700992Z","iopub.execute_input":"2024-04-15T17:47:50.701340Z","iopub.status.idle":"2024-04-15T17:47:50.706864Z","shell.execute_reply.started":"2024-04-15T17:47:50.701314Z","shell.execute_reply":"2024-04-15T17:47:50.705860Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"j_d(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:47:53.243845Z","iopub.execute_input":"2024-04-15T17:47:53.244209Z","iopub.status.idle":"2024-04-15T17:47:53.254493Z","shell.execute_reply.started":"2024-04-15T17:47:53.244180Z","shell.execute_reply":"2024-04-15T17:47:53.253567Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(0.9767661094665527, 0.976766049861908)"},"metadata":{}}]},{"cell_type":"code","source":"j_d(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:38:12.059208Z","iopub.execute_input":"2024-04-15T17:38:12.059511Z","iopub.status.idle":"2024-04-15T17:38:12.067528Z","shell.execute_reply.started":"2024-04-15T17:38:12.059483Z","shell.execute_reply":"2024-04-15T17:38:12.066507Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(tensor(0.9768), tensor(0.9882))"},"metadata":{}}]},{"cell_type":"code","source":"print(y_pred.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:32:17.234848Z","iopub.execute_input":"2024-04-15T17:32:17.235200Z","iopub.status.idle":"2024-04-15T17:32:17.240159Z","shell.execute_reply.started":"2024-04-15T17:32:17.235175Z","shell.execute_reply":"2024-04-15T17:32:17.239240Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"torch.Size([1, 240, 240])\n","output_type":"stream"}]},{"cell_type":"code","source":"num_elements","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:32:10.489085Z","iopub.execute_input":"2024-04-15T17:32:10.490022Z","iopub.status.idle":"2024-04-15T17:32:10.495611Z","shell.execute_reply.started":"2024-04-15T17:32:10.489987Z","shell.execute_reply":"2024-04-15T17:32:10.494682Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"5190"},"metadata":{}}]},{"cell_type":"code","source":"#%% visualize results\nbox_240 = useful_box_1024 / torch.asarray([1024, 1024, 1024, 1024]) * 240\nfig, ax = plt.subplots(1, 3, figsize=(10, 5))\nax[0].imshow(useful_imgs[0].cpu())\nshow_box(box_240[0].numpy(), ax[0])\nax[0].set_title(\"Input Image and Bounding Box\")\nax[1].imshow(y_pred[0])\n# show_mask(medsam_seg, ax[1])\n# show_box(box_np[0], ax[1])\nax[1].set_title(\"MedSAM Segmentation\")\n\nax[2].imshow(y_true[0])\nax[2].set_title(\"Mask\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.866338Z","iopub.status.idle":"2024-04-15T17:15:18.866657Z","shell.execute_reply.started":"2024-04-15T17:15:18.866493Z","shell.execute_reply":"2024-04-15T17:15:18.866507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.868083Z","iopub.status.idle":"2024-04-15T17:15:18.868383Z","shell.execute_reply.started":"2024-04-15T17:15:18.868235Z","shell.execute_reply":"2024-04-15T17:15:18.868247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.869495Z","iopub.status.idle":"2024-04-15T17:15:18.869844Z","shell.execute_reply.started":"2024-04-15T17:15:18.869656Z","shell.execute_reply":"2024-04-15T17:15:18.869670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Old code","metadata":{}},{"cell_type":"code","source":"# # Training loop\n# valid_batch_dice = []\n# valid_batch_jaccard = []\n# dice_metric = Dice()\n# jaccard_index_metric = BinaryJaccardIndex()\n# with torch.no_grad():\n#     H = 240\n#     W = 240\n#     for i, batch in enumerate(tqdm(val_dataloader)):\n#         # get image and masks from dataloader\n#         imgs = batch['image'].to(device).float()\n#         img_masks = batch['mask'] #.to(device).float()\n#         imgs = imgs.repeat(1, 3, 1, 1)\n#         img_masks = img_masks.repeat(1, 3, 1, 1)\n        \n#         # get useful image embeddings, bounding boxes and masks = labels\n#         image_embedding, box_1024, useful_indices = generate_box_and_embedding(imgs, img_masks)\n#         useful_box_1024 = box_1024[useful_indices,:,:]\n#         useful_image_embedding = image_embedding[useful_indices,:,:,:]\n#         y_true = img_masks[useful_indices,:,:,:]\n        \n#         print(\"image embedding shape\", useful_image_embedding.shape)\n#         print(\"box shape\", useful_box_1024.shape)\n#         print(\"mask shape\", useful_img_masks.shape)\n\n#         y_pred = torch.from_numpy(medsam_inference(medsam_model, useful_image_embedding, useful_box_1024, H, W))\n# #         y_pred = net(imgs).cpu()\n        \n#         print(\"preds shape\", y_pred.shape)\n#         print(\"mask shape\", y_true.shape)\n#         #TODO remove\n#         y_true = y_true[0][0]\n        \n#         batch_dice_score = dice_metric(y_pred, y_true)\n#         print(\"Dice\", batch_dice_score)\n#         valid_batch_dice.append(batch_dice_score)\n\n#         batch_jaccard_score = jaccard_index_metric(y_pred, y_true)\n#         valid_batch_jaccard.append(batch_jaccard_score)\n\n# #         print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {loss.item()}, DICE score: {batch_dice_score}, Jaccard score: {batch_jaccard_score}', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.870972Z","iopub.status.idle":"2024-04-15T17:15:18.871280Z","shell.execute_reply.started":"2024-04-15T17:15:18.871121Z","shell.execute_reply":"2024-04-15T17:15:18.871140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_box_and_embedding(img_np, mask_np):\n# #     slice_indexes.append(slice_ind)\n# #     if len(img_np.shape) == 2:\n# #         img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n# #     else:\n# #         img_3c = img_np\n# #     H, W, _ = img_3c.shape\n#     H = 240\n#     W = 240\n\n#     #%% image preprocessing part 2\n#     # 16,1,1024,1024\n#     resizer = torchvision.transforms.Resize(size = (1024,1024))\n#     img_1024 = resizer(img_np)\n# #     img_1024 = transform.resize(img_np, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n#     print(\"img_1024 shape: \", img_1024.shape)\n# #     img_1024 = (img_1024 - img_1024.min()) / np.clip(\n# #         img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n# #     )  # normalize to [0, 1], (H, W, 3)\n# #     # convert the shape to (3, H, W)\n# #     img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n#     # need to rewrite box coordinates from mask as input is now [16,1,240,240]\n# #     x0, y0, x1, y1 = box_coordinates_from_mask(mask_np)\n# #     box_np = np.array([[x0,y0, x1, y1]])\n# #     box_1024 = box_np / np.array([W, H, W, H]) * 1024\n#     box_1024, useless_image_indices = box_coordinates_from_mask_batch(mask_np)\n#     print(img_1024.shape)\n    \n#     mask = torch.ones(BATCH_SIZE, dtype=bool)\n#     for index in useless_image_indices:\n#         mask[index] = False\n#     # Negate the mask\n#     negated_mask = ~mask\n# #     print(\"negated mask\", negated_mask)\n# #     print(\"img_1024.shape\", img_1024.shape)\n#     filtered_img_1024_tensor = img_1024[negated_mask,:,:,:]    \n    \n#     filtered_img_1024_tensor_repeated = filtered_img_1024_tensor.repeat(1, 3, 1, 1)\n#     print(\"filtered shape\", filtered_img_1024_tensor.shape)\n#     with torch.no_grad():\n#         image_embedding = medsam_model.image_encoder(filtered_img_1024_tensor_repeated) # (1, 256, 64, 64)\n#     print(\"image_embedding shape\", image_embedding.shape)\n#     print(\"box_1024 shape\", box_1024.shape)\n#     return image_embedding, box_1024, negated_mask","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.872872Z","iopub.status.idle":"2024-04-15T17:15:18.873188Z","shell.execute_reply.started":"2024-04-15T17:15:18.873036Z","shell.execute_reply":"2024-04-15T17:15:18.873048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box_coordinates_from_mask(mask, margin = 5):\n    # Works on [240, 240]\n    # Modify for this to work on [16, 1, 240, 240]\n    nonzero_indices = np.transpose(np.nonzero(mask))\n\n    min_y = np.min(nonzero_indices[:, 0])\n    max_y = np.max(nonzero_indices[:, 0])\n    min_x = np.min(nonzero_indices[:, 1])\n    max_x = np.max(nonzero_indices[:, 1])\n\n    margin = 5\n\n    x0 = min_x - margin\n    y0 = min_y - margin\n    x1 = max_x + margin\n    y1 = max_y + margin\n\n    # print(f\"Row and column for non-zero elements with greatest x range: [{min_x}, {max_x}]\")\n    # print(f\"Row and column for non-zero elements with greatest y range: [{min_y}, {max_y}]\")\n    \n    box_np = np.array([[x0,y0, x1, y1]])\n    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n    return box_1024","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:18.874019Z","iopub.status.idle":"2024-04-15T17:15:18.874314Z","shell.execute_reply.started":"2024-04-15T17:15:18.874165Z","shell.execute_reply":"2024-04-15T17:15:18.874177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}